{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Fundamentals of Natural Language Processing**\n"
      ],
      "metadata": {
        "id": "TvoEH9xi0CsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Negation and Uncertainty Detection Project"
      ],
      "metadata": {
        "id": "u6SCLIwn1UAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing the necessary packages:**\n",
        "\n",
        " SpaCy for natural language processing, along with its Spanish and Catalan models. We’ll also install `unidecode` text cleaning"
      ],
      "metadata": {
        "id": "ySqvt6UJ1iob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (execute only if not already installed)\n",
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koK_X3oBjDm0",
        "outputId": "7d76419e-ed2b-4e83-bf5f-bf2da4704224",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install spacy unidecode\n",
        "!python -m spacy download es_core_news_sm\n",
        "!python -m spacy download ca_core_news_sm\n"
      ],
      "metadata": {
        "id": "PdpI1R56AyrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3b25ea2-3c01-4411-a558-acda7b3bb687",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting ca-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.8.0/ca_core_news_sm-3.8.0-py3-none-any.whl (19.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ca-core-news-sm\n",
            "Successfully installed ca-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ca_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on the Test Set\n",
        "We use the **`normalize_text`** function for text preprocessing.\n",
        "\n",
        "Pattern detection is improved by adding helper functions like **`detect_multiword_pattern`** to identify composed negation phrases. We also integrate four rule types into the **`analyze_medical_context`** function: prefix, postfix, UMLS_Term + Negation_Phrase, and Negation_Phrase + UMLS_Term.\n",
        "\n",
        "All detected matches are logged, counters are updated, and relevant examples are stored for the final output.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tTbrbP3shzu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing libraries for text processing, NLP, and file handling in Colab**"
      ],
      "metadata": {
        "id": "DKQsgw6v2urX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter, defaultdict\n",
        "import spacy\n",
        "from google.colab import files\n",
        "from spacy.matcher import PhraseMatcher\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n"
      ],
      "metadata": {
        "id": "-xzjIkC52vNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text normalization:**\n",
        "\n",
        "Before starting any analysis, we define a function to normalize the clinical text, removing special characters, extra spaces, and anything that might interfere with proper analysis."
      ],
      "metadata": {
        "id": "lKqxwF8224sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    \"\"\"Normalize medical text with Spanish/Catalan character support.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'\\*+', '', text)\n",
        "    # Permitir el apóstrofe (')\n",
        "    text = re.sub(r\"[^\\w\\s.,;:!?'\\-àáèéìíòóùúüñç]\", '', text)\n",
        "    return text\n",
        "def normalize_for_eval(text):\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'\\*+', '', text)\n",
        "    text = re.sub(r\"[^\\w\\s.,;:!?'\\-àáèéìíòóùúüñç]\", '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "kXABqlrs29VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will define the key elements our system will use to detect negation and uncertainty:  \n",
        "- A list of negation and uncertainty cues (including multi-word expressions)  \n",
        "- A base list of UMLS medical terms.  \n",
        "- A set of common medical suffixes to help catch additional terms not explicitly listed"
      ],
      "metadata": {
        "id": "3YwfWlW52_-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define lists of cues and UMLS medical terms\n",
        "NEGATION_WORDS = [\n",
        "    \"no\", \"sin\", \"ausencia de\", \"descarta\", \"descartado\", \"excluye\", \"excluido\", \"niega\", \"negado\",\n",
        "    \"negativa\", \"negación\", \"ningún\", \"ninguna\", \"ninguno\", \"imposible\", \"inhallable\", \"carece de\", \"nunca\",\n",
        "    \"jamás\", \"tampoco\", \"ni\", \"nada\", \"negativo\", \"mai\",\n",
        "    \"sin evidencia de\", \"no se observa\", \"no presenta\", \"no muestra\", \"no evidencia\", \"no compatible con\",\n",
        "    \"no concluyente\", \"no parece\", \"no se detecta\", \"sin signos de\", \"sin síntomas de\", \"sin indicios de\",\n",
        "    \"sin hallazgos de\", \"sin pruebas de\", \"sin rastro de\", \"ausente\", \"no encontrado\", \"sin cambios\",\n",
        "    \"no se aprecian\", \"no se ven\", \"descartando\", \"descartable\", \"no hay evidencia de\", \"no hay indicación de\",\n",
        "    \"libre de\", \"exento de\", \"sin manifestaciones de\", \"se excluye\", \"queda descartado\", \"ninguna evidencia de\",\n",
        "    \"ningún signo de\", \"sin afección\", \"no identificado\", \"negado por el paciente\", \"negado clínicamente\",\n",
        "    \"sin enfermedad\", \"sin afectación\", \"no afectado\", \"no positivo\", \"resultado negativo\",\n",
        "    \"resultado no reactivo\", \"resultado no positivo\"\n",
        "]\n",
        "\n",
        "UNCERTAINTY_WORDS = [\n",
        "    \"posible\", \"quizás\", \"podría\", \"sospecha de\", \"considera\", \"probable\", \"aparentemente\", \"puede\", \"posiblemente\",\n",
        "    \"parece\", \"se considera\", \"indeterminado\", \"probabilidad de\", \"no concluyente\", \"eventual\", \"en estudio\",\n",
        "    \"pendiente de evaluación\", \"sugestivo de\", \"sugiere\", \"indica que\", \"se sospecha de\", \"podría indicar\",\n",
        "    \"dudoso\", \"no definido\", \"no específico\", \"no determinado\", \"valor incierto\", \"no claro\", \"no seguro\",\n",
        "    \"compatible con\", \"aparenta ser\", \"tendría que evaluarse\", \"a determinar\", \"probabilidad baja de\",\n",
        "    \"probabilidad alta de\", \"sin certeza\", \"hipotético\", \"hipotéticamente\", \"a confirmar\", \"falta de certeza\",\n",
        "    \"en posible relación con\", \"estaría asociado\", \"aparentemente relacionado con\", \"se intuye\", \"se deduce que\",\n",
        "    \"en consideració\", \"probablemente\", \"tal vez\", \"aproximadamente\"\n",
        "]\n",
        "\n",
        "# Global list of UMLS medical terms (initially provided, without terms ending in a medical suffix)\n",
        "UMLS_MEDICAL_TERMS = [\n",
        "    \"interna\", \"cistoscopia\", \"uretra\", \"cronica\", \"insuficiencia\", \"renal\", \"bloqueo\", \"auriculoventricular\",\n",
        "    \"primer grado\", \"segundo grado\", \"hipertension\", \"arterial\", \"protesis\", \"cadera\", \"herniorrafia\", \"parto\",\n",
        "    \"eutocico\", \"rotura\", \"membranas\", \"prematuro\", \"lactancia materna\", \"aguda\", \"parcial\", \"angiomiolipoma\",\n",
        "    \"quistes\", \"renales\", \"fractura\", \"mandibular\", \"ictus\", \"infarto\", \"isquemico\", \"cerebral\", \"endovenosa\",\n",
        "    \"microcirugia\", \"endolaringea\", \"sensitiva\", \"axonal\", \"multifactorial\", \"déficit\"\n",
        "]\n",
        "\n",
        "# List of medical suffixes\n",
        "MEDICAL_SUFFIXES = [\n",
        "    \"nosis\", \"tomia\", \"patia\", \"losis\", \"lisis\", \"iasis\", \"scopica\", \"scopia\", \"tocico\", \"itis\",\n",
        "    \"algia\", \"oma\", \"emia\", \"plegia\", \"penia\", \"cele\", \"plasia\", \"ectasia\", \"uria\"\n",
        "]\n",
        "\n",
        "# === Build PhraseMatchers ===\n",
        "neg_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "neg_patterns = [nlp.make_doc(cue) for cue in NEGATION_WORDS]\n",
        "neg_matcher.add(\"NEGATION\", neg_patterns)\n",
        "\n",
        "unc_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "unc_patterns = [nlp.make_doc(cue) for cue in UNCERTAINTY_WORDS]\n",
        "unc_matcher.add(\"UNCERTAINTY\", unc_patterns)"
      ],
      "metadata": {
        "id": "I4jiOS863ANL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper functions for detection and evaluation**  \n",
        "Next, we define our three utility functions:  \n",
        "1. One to detect double negation in a context window.  \n",
        "2. One to extract ground truth labels (`NEG` and `UNC`) from the annotated dataset.  \n",
        "3. And one to identify terms based on typical medical suffixes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hMUUz8U93zQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def is_double_negation(tokens):\n",
        "    \"\"\"Check if a sequence of tokens contains double negation.\"\"\"\n",
        "    simple_negation_cues = {\"no\", \"sin\", \"nunca\", \"jamás\", \"ningún\", \"ninguna\", \"nadie\", \"ninguno\", \"negado\", \"niega\"}\n",
        "    negation_count = sum(1 for token in tokens if token in simple_negation_cues)\n",
        "    return negation_count >= 2\n",
        "\n",
        "def extract_ground_truth(record, record_index):\n",
        "    gt_neg = set()\n",
        "    gt_unc = set()\n",
        "\n",
        "    entries = []\n",
        "    if record.get(\"annotations\"):\n",
        "        entries = record[\"annotations\"]\n",
        "    elif record.get(\"predictions\"):\n",
        "        entries = record[\"predictions\"]\n",
        "\n",
        "    for ann in entries:\n",
        "        for res in ann.get(\"result\", []):\n",
        "            labels = res.get(\"value\", {}).get(\"labels\", [])\n",
        "            start = res.get(\"value\", {}).get(\"start\", None)\n",
        "            end = res.get(\"value\", {}).get(\"end\", None)\n",
        "            text_val = res.get(\"value\", {}).get(\"text\", \"\").lower()\n",
        "            if start is not None and end is not None:\n",
        "                if \"NEG\" in labels:\n",
        "                    gt_neg.add((record_index, start, end, text_val))\n",
        "                if \"UNC\" in labels:\n",
        "                    gt_unc.add((record_index, start, end, text_val))\n",
        "    return gt_neg, gt_unc\n",
        "\n",
        "\n",
        "def detect_medical_suffix(word):\n",
        "    \"\"\"Return True if the word ends with any of the defined medical suffixes.\"\"\"\n",
        "    for suffix in MEDICAL_SUFFIXES:\n",
        "        if word.endswith(suffix):\n",
        "            return True\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "kGKDuNhD3zch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main analysis Function**  \n",
        "This function willbring everything together. It will go through each sentence, looking for negation or uncertainty cues, and analyzing the surrounding context to detect relevant medical terms either from the UMLS list or by matching medical suffixes.\n",
        "\n",
        "It also groups detections by context, flags double negation when found, and keeps track of everything for evaluation later on.\n"
      ],
      "metadata": {
        "id": "MRhITvQY4OzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Normalize UMLS medical terms to lowercase lemmas\n",
        "MEDICAL_LEMMA_SET = set([term.lower() for term in UMLS_MEDICAL_TERMS])\n",
        "\n",
        "def analyze_medical_context(text, nlp, neg_matcher, unc_matcher):\n",
        "    \"\"\"\n",
        "    Uses PhraseMatchers to detect negation and uncertainty cues in context windows.\n",
        "    Expands with bigram medical term detection and increased context window.\n",
        "    \"\"\"\n",
        "    normalized_text = normalize_text(text)\n",
        "    doc = nlp(normalized_text)\n",
        "    results = {\n",
        "        \"negated_terms_grouped\": [],\n",
        "        \"uncertain_terms_grouped\": [],\n",
        "        \"double_negated_terms\": [],\n",
        "        \"negation_cues_used\": Counter(),\n",
        "        \"uncertainty_cues_used\": Counter(),\n",
        "        \"medical_terms_found\": Counter(),\n",
        "        \"medical_suffix_terms\": []\n",
        "    }\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        tokens = list(sent)\n",
        "        lemmas = [token.lemma_.lower() for token in tokens]\n",
        "\n",
        "        # NEGATION MATCHER\n",
        "        for match_id, start, end in neg_matcher(sent):\n",
        "            cue = sent[start:end].text.lower()\n",
        "            window_start = max(0, start - 15)\n",
        "            window_end = min(len(tokens), end + 15)\n",
        "            context = tokens[window_start:window_end]\n",
        "            context_text = \" \".join([t.text for t in context])\n",
        "            detections = []\n",
        "\n",
        "            for token in context:\n",
        "                term = token.lemma_.lower()\n",
        "                if token.is_stop or token.like_num:\n",
        "                    continue\n",
        "                if term in MEDICAL_LEMMA_SET or detect_medical_suffix(term):\n",
        "                    detection = {\n",
        "                        \"term\": term,\n",
        "                        \"detected_text\": token.text,\n",
        "                        \"start\": token.idx,\n",
        "                        \"end\": token.idx + len(token.text)\n",
        "                    }\n",
        "                    detections.append(detection)\n",
        "                    results[\"medical_terms_found\"][term] += 1\n",
        "                    if detect_medical_suffix(term) and term not in MEDICAL_LEMMA_SET:\n",
        "                        results[\"medical_suffix_terms\"].append(detection)\n",
        "\n",
        "            # BIGRAM DETECTION\n",
        "            for j in range(len(context) - 1):\n",
        "                bigram = f\"{context[j].text.lower()} {context[j+1].text.lower()}\"\n",
        "                if bigram in MEDICAL_LEMMA_SET:\n",
        "                    detection = {\n",
        "                        \"term\": bigram,\n",
        "                        \"detected_text\": f\"{context[j].text} {context[j+1].text}\",\n",
        "                        \"start\": context[j].idx,\n",
        "                        \"end\": context[j+1].idx + len(context[j+1].text)\n",
        "                    }\n",
        "                    detections.append(detection)\n",
        "                    results[\"medical_terms_found\"][bigram] += 1\n",
        "\n",
        "            if detections:\n",
        "                group = {\n",
        "                    \"cue\": cue,\n",
        "                    \"context\": context_text,\n",
        "                    \"window_start\": context[0].idx,\n",
        "                    \"window_end\": context[-1].idx + len(context[-1].text),\n",
        "                    \"detections\": detections\n",
        "                }\n",
        "                if is_double_negation(lemmas[window_start:window_end]):\n",
        "                    results[\"double_negated_terms\"].append(group)\n",
        "                else:\n",
        "                    results[\"negated_terms_grouped\"].append(group)\n",
        "                results[\"negation_cues_used\"][cue] += len(detections)\n",
        "\n",
        "        # UNCERTAINTY MATCHER\n",
        "        for match_id, start, end in unc_matcher(sent):\n",
        "            cue = sent[start:end].text.lower()\n",
        "            window_start = max(0, start - 15)\n",
        "            window_end = min(len(tokens), end + 15)\n",
        "            context = tokens[window_start:window_end]\n",
        "            context_text = \" \".join([t.text for t in context])\n",
        "            detections = []\n",
        "\n",
        "            for token in context:\n",
        "                term = token.lemma_.lower()\n",
        "                if token.is_stop or token.like_num:\n",
        "                    continue\n",
        "                if term in MEDICAL_LEMMA_SET or detect_medical_suffix(term):\n",
        "                    detection = {\n",
        "                        \"term\": term,\n",
        "                        \"detected_text\": token.text,\n",
        "                        \"start\": token.idx,\n",
        "                        \"end\": token.idx + len(token.text)\n",
        "                    }\n",
        "                    detections.append(detection)\n",
        "                    results[\"medical_terms_found\"][term] += 1\n",
        "\n",
        "            # BIGRAM DETECTION\n",
        "            for j in range(len(context) - 1):\n",
        "                bigram = f\"{context[j].text.lower()} {context[j+1].text.lower()}\"\n",
        "                if bigram in MEDICAL_LEMMA_SET:\n",
        "                    detection = {\n",
        "                        \"term\": bigram,\n",
        "                        \"detected_text\": f\"{context[j].text} {context[j+1].text}\",\n",
        "                        \"start\": context[j].idx,\n",
        "                        \"end\": context[j+1].idx + len(context[j+1].text)\n",
        "                    }\n",
        "                    detections.append(detection)\n",
        "                    results[\"medical_terms_found\"][bigram] += 1\n",
        "\n",
        "            if detections:\n",
        "                group = {\n",
        "                    \"cue\": cue,\n",
        "                    \"context\": context_text,\n",
        "                    \"window_start\": context[0].idx,\n",
        "                    \"window_end\": context[-1].idx + len(context[-1].text),\n",
        "                    \"detections\": detections\n",
        "                }\n",
        "                results[\"uncertainty_cues_used\"][cue] += len(detections)\n",
        "                results[\"uncertain_terms_grouped\"].append(group)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "y6EsTrsw4UxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation metrics:**\n",
        "\n",
        "We will now impkement a function to calculate precision, recall, and F1 score. Which are basic but essential metrics to evaluate how well our system detects negation and uncertainty compared to the annotated ground truth"
      ],
      "metadata": {
        "id": "B4LuSmKP4ane"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(predicted, ground_truth):\n",
        "    tp = len(predicted & ground_truth)\n",
        "    fp = len(predicted - ground_truth)\n",
        "    fn = len(ground_truth - predicted)\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "    jaccard = tp / len(predicted | ground_truth) if (predicted | ground_truth) else 0.0\n",
        "\n",
        "    return precision, recall, f1, jaccard\n"
      ],
      "metadata": {
        "id": "FqABvcgr4avk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final execution  evaluation and export**  \n",
        "Great! Now we that we have everything we need. Lets implement the main function that will run the entire pipeline on the test set. It loads the model and data, processes each record, applies the detection logic, and updates statistics and evaluation metrics for negation and uncertainty.\n",
        "\n",
        "It also prints out cue frequencies, example detections, and performance results (precision, recall, F1, and Jaccard accuracy).  \n",
        "At the end, it saves the structured output to a JSON file and makes it available for download.\n"
      ],
      "metadata": {
        "id": "iKLCG2zV5bQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_phrase_matcher(nlp, cue_list):\n",
        "    matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
        "    patterns = [nlp.make_doc(cue) for cue in cue_list]\n",
        "    matcher.add(\"CUES\", patterns)\n",
        "    return matcher\n",
        "\n",
        "def main():\n",
        "    import time\n",
        "    print(\"Loading SpaCy model...\")\n",
        "    nlp = spacy.load(\"es_core_news_sm\")\n",
        "    neg_matcher = build_phrase_matcher(nlp, NEGATION_WORDS)\n",
        "    unc_matcher = build_phrase_matcher(nlp, UNCERTAINTY_WORDS)\n",
        "\n",
        "    print(\"Please upload your JSON file...\")\n",
        "    uploaded = files.upload()\n",
        "    filename = next(iter(uploaded))\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    total_negation_counter = Counter()\n",
        "    total_uncertainty_counter = Counter()\n",
        "    total_medical_counter = Counter()\n",
        "    negated_examples = []\n",
        "    uncertain_examples = []\n",
        "\n",
        "    # Conjuntos para evaluación: (record_index, start, end, text)\n",
        "    all_predicted_neg = set()\n",
        "    all_ground_truth_neg = set()\n",
        "    all_predicted_unc = set()\n",
        "    all_ground_truth_unc = set()\n",
        "\n",
        "    records_with_neg = []\n",
        "    records_with_unc = []\n",
        "    output_data = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(f\"Processing {len(data)} records...\")\n",
        "    for i, record in enumerate(data):\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Processing record {i+1}/{len(data)}...\")\n",
        "        text = record.get(\"data\", {}).get(\"text\", \"\")\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        results = analyze_medical_context(text, nlp, neg_matcher, unc_matcher)\n",
        "        total_negation_counter.update(results[\"negation_cues_used\"])\n",
        "        total_uncertainty_counter.update(results[\"uncertainty_cues_used\"])\n",
        "        total_medical_counter.update(results[\"medical_terms_found\"])\n",
        "\n",
        "        if results.get(\"negated_terms_grouped\"):\n",
        "            records_with_neg.append(i)\n",
        "        if results.get(\"uncertain_terms_grouped\"):\n",
        "            records_with_unc.append(i)\n",
        "\n",
        "        # Procesar detecciones agrupadas para negación\n",
        "        if \"negated_terms_grouped\" in results:\n",
        "            for group in results[\"negated_terms_grouped\"]:\n",
        "                terms_info = \"; \".join([f\"{d['term']} (detected: {d['detected_text']})\" for d in group[\"detections\"]])\n",
        "                if len(negated_examples) < 20:\n",
        "                    negated_examples.append(f\"Record {i}: Context: '{group['context']}' (cue: {group['cue']}, terms: {terms_info}, window: {group['window_start']}-{group['window_end']})\")\n",
        "                    # Usar el contexto agrupado como clave para evaluación\n",
        "                    normalized_cue = normalize_for_eval(group[\"cue\"])\n",
        "                    all_predicted_neg.add((i, normalized_cue))\n",
        "\n",
        "        # Procesar detecciones agrupadas para incertidumbre\n",
        "        if \"uncertain_terms_grouped\" in results:\n",
        "            for group in results[\"uncertain_terms_grouped\"]:\n",
        "                terms_info = \"; \".join([f\"{d['term']} (detected: {d['detected_text']})\" for d in group[\"detections\"]])\n",
        "                if len(uncertain_examples) < 20:\n",
        "                    uncertain_examples.append(f\"Record {i}: Context: '{group['context']}' (cue: {group['cue']}, terms: {terms_info}, window: {group['window_start']}-{group['window_end']})\")\n",
        "                    normalized_cue = normalize_for_eval(group[\"cue\"])\n",
        "                    all_predicted_unc.add((i, normalized_cue))\n",
        "\n",
        "        # Extraer ground truth (NEG y UNC) desde las anotaciones o predicciones en el JSON\n",
        "        gt_neg, gt_unc = extract_ground_truth(record, i)\n",
        "        all_ground_truth_neg.update(gt_neg)\n",
        "        all_ground_truth_unc.update(gt_unc)\n",
        "\n",
        "        output_record = {\n",
        "            \"id\": record.get(\"id\", None),\n",
        "            \"record_index\": i,\n",
        "            \"predicted\": {\n",
        "                \"negated_terms_grouped\": results.get(\"negated_terms_grouped\", []),\n",
        "                \"uncertain_terms_grouped\": results.get(\"uncertain_terms_grouped\", []),\n",
        "                \"double_negated_terms\": results.get(\"double_negated_terms\", []),\n",
        "                \"medical_suffix_terms\": results.get(\"medical_suffix_terms\", [])\n",
        "            }\n",
        "        }\n",
        "        output_data.append(output_record)\n",
        "\n",
        "    end_time = time.time()\n",
        "    processing_time = end_time - start_time\n",
        "    print(\" Ground truth samples:\")\n",
        "    for i, item in enumerate(sorted(all_ground_truth_neg)[:3]):\n",
        "        print(f\"GT: {item}\")\n",
        "\n",
        "    print(\"\\n Predicted samples:\")\n",
        "    for i, item in enumerate(sorted(all_predicted_neg)[:3]):\n",
        "        print(f\"PRED: {item}\")\n",
        "\n",
        "    print(\"\\nINTERSECTION:\")\n",
        "    print(all_predicted_neg & all_ground_truth_neg)\n",
        "\n",
        "\n",
        "    # Now compute metrics as usual\n",
        "    prec_neg, rec_neg, f1_neg, acc_neg = compute_metrics(all_predicted_neg, all_ground_truth_neg)\n",
        "\n",
        "    # Calcular métricas para negación, incertidumbre y combinadas\n",
        "    prec_neg, rec_neg, f1_neg, acc_neg = compute_metrics_loose_match(all_predicted_neg, all_ground_truth_neg)\n",
        "    prec_unc, rec_unc, f1_unc, acc_unc = compute_metrics_loose_match(all_predicted_unc, all_ground_truth_unc)\n",
        "\n",
        "    combined_predicted = all_predicted_neg.union(all_predicted_unc)\n",
        "    combined_ground_truth = all_ground_truth_neg.union(all_ground_truth_unc)\n",
        "    prec_comb, rec_comb, f1_comb, acc_comb = compute_metrics_loose_match(combined_predicted, combined_ground_truth)\n",
        "\n",
        "\n",
        "    print(\"\\n=== STATISTICS ===\")\n",
        "    print(\"\\nNegation Cue Frequencies (affecting medical terms):\")\n",
        "    for word, freq in total_negation_counter.most_common():\n",
        "        print(f\"{word}: {freq}\")\n",
        "    print(\"\\nUncertainty Cue Frequencies (affecting medical terms):\")\n",
        "    for word, freq in total_uncertainty_counter.most_common():\n",
        "        print(f\"{word}: {freq}\")\n",
        "    print(\"\\nMedical Terms Frequencies:\")\n",
        "    for word, freq in total_medical_counter.most_common():\n",
        "        print(f\"{word}: {freq}\")\n",
        "\n",
        "    print(\"\\n=== EXAMPLES OF NEGATED MEDICAL TERMS ===\")\n",
        "    for example in negated_examples:\n",
        "        print(example)\n",
        "    print(\"\\n=== EXAMPLES OF UNCERTAIN MEDICAL TERMS ===\")\n",
        "    for example in uncertain_examples:\n",
        "        print(example)\n",
        "\n",
        "    print(\"\\n=== TOTAL DETECTED CONTEXTS ===\")\n",
        "    print(f\"Total detected negated contexts: {len(all_predicted_neg)}\")\n",
        "    print(f\"Total detected uncertain contexts: {len(all_predicted_unc)}\")\n",
        "    print(f\"Total combined detected contexts: {len(combined_predicted)}\")\n",
        "\n",
        "    print(\"\\n=== RECORDS WITH DETECTIONS ===\")\n",
        "    print(f\"Records with negation detections: {records_with_neg}\")\n",
        "    print(f\"Records with uncertainty detections: {records_with_unc}\")\n",
        "\n",
        "    print(\"\\n=== EFFICIENCY ===\")\n",
        "    print(f\"Total processing time: {processing_time:.2f} seconds\")\n",
        "\n",
        "    print(\"\\n=== EVALUATION METRICS ===\")\n",
        "    print(\"\\nNegation Metrics:\")\n",
        "    print(f\"Precision: {prec_neg:.2f}, Recall: {rec_neg:.2f}, F1: {f1_neg:.2f}, Accuracy (Jaccard): {acc_neg:.2f}\")\n",
        "    print(f\"\\n Matches found using cue containment! TP: {int(prec_comb * (len(combined_predicted)))}\")\n",
        "\n",
        "    print(\"\\nUncertainty Metrics:\")\n",
        "    print(f\"Precision: {prec_unc:.2f}, Recall: {rec_unc:.2f}, F1: {f1_unc:.2f}, Accuracy (Jaccard): {acc_unc:.2f}\")\n",
        "    print(f\"Matched uncertainty cues (TP estimate): {int(prec_unc * len(all_predicted_unc))}\")\n",
        "\n",
        "    print(\"\\nCombined (Negation + Uncertainty) Metrics:\")\n",
        "    print(f\"Precision: {prec_comb:.2f}, Recall: {rec_comb:.2f}, F1: {f1_comb:.2f}, Accuracy (Jaccard): {acc_comb:.2f}\")\n",
        "    print(f\"Matched total cues (TP estimate): {int(prec_comb * len(combined_predicted))}\")\n",
        "\n",
        "    output_filename = \"output.json\"\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as outfile:\n",
        "        json.dump(output_data, outfile, ensure_ascii=False, indent=2)\n",
        "    print(f\"\\nOutput file '{output_filename}' generated.\")\n",
        "    files.download(output_filename)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "oXK2hlkOg8ry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b10182c6-fd28-42b7-c715-34d0f91cb495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SpaCy model...\n",
            "Please upload your JSON file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-af8824bb-aff1-44d5-80d0-3626dedfab22\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-af8824bb-aff1-44d5-80d0-3626dedfab22\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving negacio_test_v2024.json to negacio_test_v2024 (1).json\n",
            "Processing 64 records...\n",
            "Processing record 1/64...\n",
            "Processing record 11/64...\n",
            "Processing record 21/64...\n",
            "Processing record 31/64...\n",
            "Processing record 41/64...\n",
            "Processing record 51/64...\n",
            "Processing record 61/64...\n",
            " Ground truth samples:\n",
            "GT: (0, 395, 398, '')\n",
            "GT: (0, 499, 505, '')\n",
            "GT: (0, 1111, 1119, '')\n",
            "\n",
            " Predicted samples:\n",
            "PRED: (0, 'niega')\n",
            "PRED: (0, 'no')\n",
            "PRED: (10, 'niega')\n",
            "\n",
            "INTERSECTION:\n",
            "set()\n",
            "\n",
            "=== STATISTICS ===\n",
            "\n",
            "Negation Cue Frequencies (affecting medical terms):\n",
            "no: 26\n",
            "niega: 8\n",
            "ni: 2\n",
            "sin: 2\n",
            "\n",
            "Uncertainty Cue Frequencies (affecting medical terms):\n",
            "probable: 2\n",
            "sospecha de: 1\n",
            "\n",
            "Medical Terms Frequencies:\n",
            "parto: 14\n",
            "apendicectomia: 4\n",
            "renal: 3\n",
            "neoplasia: 3\n",
            "arterial: 3\n",
            "protesis: 2\n",
            "adenoidectomia: 2\n",
            "hipertension: 2\n",
            "colecistectomia: 2\n",
            "colecistitis: 1\n",
            "prematuro: 1\n",
            "ictus: 1\n",
            "fractura: 1\n",
            "mandibular: 1\n",
            "ureterorrenoscopia: 1\n",
            "\n",
            "=== EXAMPLES OF NEGATED MEDICAL TERMS ===\n",
            "Record 0: Context: 'motiu d'ingres induccion al parto por pequeño para la edad gestacional   peg   antecedents no alergias medicamentosas conocidas antcededentes medico-quirurgicos : protesis mamaria , adenoidectomia niega habitos toxicos medicacio habitual' (cue: no, terms: parto (detected: parto); protesis (detected: protesis); adenoidectomia (detected: adenoidectomia), window: 234-467)\n",
            "Record 0: Context: '  peg   antecedents no alergias medicamentosas conocidas antcededentes medico-quirurgicos : protesis mamaria , adenoidectomia niega habitos toxicos medicacio habitual anafranil25 mg diario .' (cue: niega, terms: protesis (detected: protesis); adenoidectomia (detected: adenoidectomia), window: 305-490)\n",
            "Record 10: Context: 'ates per , ; ,   informe d'alta d'hospitalitzacio motiu d'ingres trabajo de parto antecedents niega alergias conocidas niega habitos toxicos niega i.q ap : calculos renales tratamiento actual : hierro' (cue: niega, terms: parto (detected: parto); renal (detected: renales), window: 194-391)\n",
            "Record 10: Context: '; ,   informe d'alta d'hospitalitzacio motiu d'ingres trabajo de parto antecedents niega alergias conocidas niega habitos toxicos niega i.q ap : calculos renales tratamiento actual : hierro proces actual edad' (cue: niega, terms: parto (detected: parto); renal (detected: renales), window: 205-410)\n",
            "Record 10: Context: 'informe d'alta d'hospitalitzacio motiu d'ingres trabajo de parto antecedents niega alergias conocidas niega habitos toxicos niega i.q ap : calculos renales tratamiento actual : hierro proces actual edad : 24 años' (cue: niega, terms: parto (detected: parto); renal (detected: renales), window: 210-419)\n",
            "Record 15: Context: 'busquet , enric informe d'alta d'hospitalitzacio motiu d'ingres colecistitis aguda antecedents -sin alergias medicamentosas conocidas ni habitos toxicos .' (cue: ni, terms: colecistitis (detected: colecistitis), window: 209-361)\n",
            "Record 20: Context: 'per , ; , ; ,   informe d'alta d'hospitalitzacio motiu d'ingres neoplasia mama antecedents no amc vhc desde la infancia .' (cue: no, terms: neoplasia (detected: neoplasia), window: 213-332)\n",
            "Record 25: Context: 'motiu d'ingres fx pertrocanterea antecedents al.lergies sense allergies conegudes ap : hipertension arterial , resto no especificados , probable alteracion de memoria.no familiar durante mi visita ssf : deambula con ayuda' (cue: no, terms: hipertension (detected: hipertension); arterial (detected: arterial), window: 265-482)\n",
            "Record 32: Context: ',   informe d'alta d'hospitalitzacio motiu d'ingres trabajo parto   ruptura prematura de membranas antecedents no alergias medicamentosas conocidas .' (cue: no, terms: parto (detected: parto); prematuro (detected: prematura), window: 207-353)\n",
            "Record 33: Context: 'per , ; , ; ,   informe d'alta d'hospitalitzacio motiu d'ingres neoplasia mama antecedents no amc vhc desde la infancia .' (cue: no, terms: neoplasia (detected: neoplasia), window: 213-332)\n",
            "Record 34: Context: 'per , ; , ; ,   informe d'alta d'hospitalitzacio motiu d'ingres neoplasia mama antecedents no amc vhc desde la infancia .' (cue: no, terms: neoplasia (detected: neoplasia), window: 213-332)\n",
            "Record 41: Context: 'ates per , ; ,   informe d'alta d'hospitalitzacio motiu d'ingres rpm a termino antecedents no alergias emdicamentosas conocidas intervenciones quirurgicas : 1 cesarea en 2011 , colecistectomia antecentes patologicos :' (cue: no, terms: colecistectomia (detected: colecistectomia), window: 201-414)\n",
            "Record 41: Context: ', colecistectomia antecentes patologicos : obesidad   imc : 41 .8   malabsorcion sales biliares no habitos toxicos .' (cue: no, terms: colecistectomia (detected: colecistectomia), window: 373-484)\n",
            "Record 51: Context: '13:18:48 ates per , ; ,   informe d'alta d'hospitalitzacio motiu d'ingres fractura mandibular antecedents sin amc antecedentes patologicos : depresion en tratamiento medico proces actual paciente que acude derivado de' (cue: sin, terms: fractura (detected: fractura); mandibular (detected: mandibular), window: 180-395)\n",
            "Record 56: Context: 'ates per , ; ,   informe d'alta d'hospitalitzacio motiu d'ingres trabajo de parto antecedents no alergias medicamentosas conocidas .' (cue: no, terms: parto (detected: parto), window: 194-324)\n",
            "Record 58: Context: '  informe d'alta d'hospitalitzacio motiu d'ingres paciente ingresa para nuevo intento de ureterorrenoscopia flexible antecedents no alergias medicamentosas conocidas .' (cue: no, terms: ureterorrenoscopia (detected: ureterorrenoscopia), window: 207-372)\n",
            "Record 60: Context: '; , ; ; ,   informe d'alta d'hospitalitzacio motiu d'ingres trabajo de parto antecedents no alergias medicamentosas conocidas .' (cue: no, terms: parto (detected: parto), window: 205-330)\n",
            "Record 63: Context: '  informe de trasllat a sociosanitari motiu d'ingres varon de 80 años sin habitos toxicos ni alergias medicamentosas conocidas que es derivado de h. san rafael por disnea antecedents -hipertension arterial' (cue: ni, terms: arterial (detected: arterial), window: 211-415)\n",
            "\n",
            "=== EXAMPLES OF UNCERTAIN MEDICAL TERMS ===\n",
            "Record 25: Context: 'pertrocanterea antecedents al.lergies sense allergies conegudes ap : hipertension arterial , resto no especificados , probable alteracion de memoria.no familiar durante mi visita ssf : deambula con ayuda de otra persona' (cue: probable, terms: hipertension (detected: hipertension); arterial (detected: arterial), window: 283-498)\n",
            "Record 50: Context: 'per , , , ; ,   informe d'alta d'hospitalitzacio motiu d'ingres paciente derivada por sospecha de ictus .' (cue: sospecha de, terms: ictus (detected: ictus), window: 193-296)\n",
            "\n",
            "=== TOTAL DETECTED CONTEXTS ===\n",
            "Total detected negated contexts: 15\n",
            "Total detected uncertain contexts: 2\n",
            "Total combined detected contexts: 17\n",
            "\n",
            "=== RECORDS WITH DETECTIONS ===\n",
            "Records with negation detections: [0, 10, 15, 20, 25, 32, 33, 34, 41, 51, 56, 58, 60, 63]\n",
            "Records with uncertainty detections: [25, 50]\n",
            "\n",
            "=== EFFICIENCY ===\n",
            "Total processing time: 9.30 seconds\n",
            "\n",
            "=== EVALUATION METRICS ===\n",
            "\n",
            "Negation Metrics:\n",
            "Precision: 0.48, Recall: 0.01, F1: 0.02, Accuracy (Jaccard): 0.01\n",
            "\n",
            " Matches found using cue containment! TP: 7\n",
            "\n",
            "Uncertainty Metrics:\n",
            "Precision: 0.50, Recall: 0.02, F1: 0.03, Accuracy (Jaccard): 0.02\n",
            "Matched uncertainty cues (TP estimate): 1\n",
            "\n",
            "Combined (Negation + Uncertainty) Metrics:\n",
            "Precision: 0.47, Recall: 0.01, F1: 0.02, Accuracy (Jaccard): 0.01\n",
            "Matched total cues (TP estimate): 7\n",
            "\n",
            "Output file 'output.json' generated.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6317ce24-effc-46b1-b95c-fec184844351\", \"output.json\", 32547)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}