{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Fundamentals of Natural Language Processing**\n"
      ],
      "metadata": {
        "id": "TvoEH9xi0CsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Negation and Uncertainty Detection Project"
      ],
      "metadata": {
        "id": "u6SCLIwn1UAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing the necessary packages:**\n",
        "\n",
        " SpaCy for natural language processing, along with its Spanish and Catalan models. We’ll also install `unidecode` text cleaning"
      ],
      "metadata": {
        "id": "ySqvt6UJ1iob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (execute only if not already installed)\n",
        "!pip install spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koK_X3oBjDm0",
        "outputId": "f149f6b5-5fa3-4ff5-cd11-24759191fbaa",
        "collapsed": true
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install spacy unidecode\n",
        "!python -m spacy download es_core_news_sm\n",
        "!python -m spacy download ca_core_news_sm\n"
      ],
      "metadata": {
        "id": "PdpI1R56AyrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e6df78-0da2-41f5-fbbe-169c8b16bf8a",
        "collapsed": true
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (1.3.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting ca-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.8.0/ca_core_news_sm-3.8.0-py3-none-any.whl (19.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ca_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on the Test Set\n",
        "We use the **`normalize_text`** function for text preprocessing.\n",
        "\n",
        "Pattern detection is improved by adding helper functions like **`detect_multiword_pattern`** to identify composed negation phrases. We also integrate four rule types into the **`analyze_medical_context`** function: prefix, postfix, UMLS_Term + Negation_Phrase, and Negation_Phrase + UMLS_Term.\n",
        "\n",
        "All detected matches are logged, counters are updated, and relevant examples are stored for the final output.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tTbrbP3shzu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing libraries for text processing, NLP, and file handling in Colab**"
      ],
      "metadata": {
        "id": "DKQsgw6v2urX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter, defaultdict\n",
        "import spacy\n",
        "from google.colab import files\n",
        "\n"
      ],
      "metadata": {
        "id": "-xzjIkC52vNL"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text normalization:**\n",
        "\n",
        "Before starting any analysis, we define a function to normalize the clinical text, removing special characters, extra spaces, and anything that might interfere with proper analysis."
      ],
      "metadata": {
        "id": "lKqxwF8224sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    \"\"\"Normalize medical text with Spanish/Catalan character support.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'\\*+', '', text)\n",
        "    # Permitir el apóstrofe (')\n",
        "    text = re.sub(r\"[^\\w\\s.,;:!?'\\-àáèéìíòóùúüñç]\", '', text)\n",
        "    return text\n",
        "def normalize_for_eval(text):\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'\\*+', '', text)\n",
        "    text = re.sub(r\"[^\\w\\s.,;:!?'\\-àáèéìíòóùúüñç]\", '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "kXABqlrs29VC"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will define the key elements our system will use to detect negation and uncertainty:  \n",
        "- A list of negation and uncertainty cues (including multi-word expressions)  \n",
        "- A base list of UMLS medical terms.  \n",
        "- A set of common medical suffixes to help catch additional terms not explicitly listed"
      ],
      "metadata": {
        "id": "3YwfWlW52_-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define lists of cues and UMLS medical terms\n",
        "NEGATION_WORDS = [\n",
        "    \"no\", \"sin\", \"ausencia de\", \"descarta\", \"descartado\", \"excluye\", \"excluido\", \"niega\", \"negado\",\n",
        "    \"negativa\", \"negación\", \"ningún\", \"ninguna\", \"ninguno\", \"imposible\", \"inhallable\", \"carece de\", \"nunca\",\n",
        "    \"jamás\", \"tampoco\", \"ni\", \"nada\", \"negativo\", \"mai\",\n",
        "    \"sin evidencia de\", \"no se observa\", \"no presenta\", \"no muestra\", \"no evidencia\", \"no compatible con\",\n",
        "    \"no concluyente\", \"no parece\", \"no se detecta\", \"sin signos de\", \"sin síntomas de\", \"sin indicios de\",\n",
        "    \"sin hallazgos de\", \"sin pruebas de\", \"sin rastro de\", \"ausente\", \"no encontrado\", \"sin cambios\",\n",
        "    \"no se aprecian\", \"no se ven\", \"descartando\", \"descartable\", \"no hay evidencia de\", \"no hay indicación de\",\n",
        "    \"libre de\", \"exento de\", \"sin manifestaciones de\", \"se excluye\", \"queda descartado\", \"ninguna evidencia de\",\n",
        "    \"ningún signo de\", \"sin afección\", \"no identificado\", \"negado por el paciente\", \"negado clínicamente\",\n",
        "    \"sin enfermedad\", \"sin afectación\", \"no afectado\", \"no positivo\", \"resultado negativo\",\n",
        "    \"resultado no reactivo\", \"resultado no positivo\"\n",
        "]\n",
        "\n",
        "UNCERTAINTY_WORDS = [\n",
        "    \"posible\", \"quizás\", \"podría\", \"sospecha de\", \"considera\", \"probable\", \"aparentemente\", \"puede\", \"posiblemente\",\n",
        "    \"parece\", \"se considera\", \"indeterminado\", \"probabilidad de\", \"no concluyente\", \"eventual\", \"en estudio\",\n",
        "    \"pendiente de evaluación\", \"sugestivo de\", \"sugiere\", \"indica que\", \"se sospecha de\", \"podría indicar\",\n",
        "    \"dudoso\", \"no definido\", \"no específico\", \"no determinado\", \"valor incierto\", \"no claro\", \"no seguro\",\n",
        "    \"compatible con\", \"aparenta ser\", \"tendría que evaluarse\", \"a determinar\", \"probabilidad baja de\",\n",
        "    \"probabilidad alta de\", \"sin certeza\", \"hipotético\", \"hipotéticamente\", \"a confirmar\", \"falta de certeza\",\n",
        "    \"en posible relación con\", \"estaría asociado\", \"aparentemente relacionado con\", \"se intuye\", \"se deduce que\",\n",
        "    \"en consideració\", \"probablemente\", \"tal vez\", \"aproximadamente\"\n",
        "]\n",
        "\n",
        "# Global list of UMLS medical terms (initially provided, without terms ending in a medical suffix)\n",
        "UMLS_MEDICAL_TERMS = [\n",
        "    \"interna\", \"cistoscopia\", \"uretra\", \"cronica\", \"insuficiencia\", \"renal\", \"bloqueo\", \"auriculoventricular\",\n",
        "    \"primer grado\", \"segundo grado\", \"hipertension\", \"arterial\", \"protesis\", \"cadera\", \"herniorrafia\", \"parto\",\n",
        "    \"eutocico\", \"rotura\", \"membranas\", \"prematuro\", \"lactancia materna\", \"aguda\", \"parcial\", \"angiomiolipoma\",\n",
        "    \"quistes\", \"renales\", \"fractura\", \"mandibular\", \"ictus\", \"infarto\", \"isquemico\", \"cerebral\", \"endovenosa\",\n",
        "    \"microcirugia\", \"endolaringea\", \"sensitiva\", \"axonal\", \"multifactorial\", \"déficit\"\n",
        "]\n",
        "\n",
        "# List of medical suffixes\n",
        "MEDICAL_SUFFIXES = [\n",
        "    \"nosis\", \"tomia\", \"patia\", \"losis\", \"lisis\", \"iasis\", \"scopica\", \"scopia\", \"tocico\", \"itis\",\n",
        "    \"algia\", \"oma\", \"emia\", \"plegia\", \"penia\", \"cele\", \"plasia\", \"ectasia\", \"uria\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "I4jiOS863ANL"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper functions for detection and evaluation**  \n",
        "Next, we define our three utility functions:  \n",
        "1. One to detect double negation in a context window.  \n",
        "2. One to extract ground truth labels (`NEG` and `UNC`) from the annotated dataset.  \n",
        "3. And one to identify terms based on typical medical suffixes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hMUUz8U93zQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def is_double_negation(tokens):\n",
        "    \"\"\"Check if a sequence of tokens contains double negation.\"\"\"\n",
        "    negation_count = sum(1 for token in tokens if token in NEGATION_WORDS)\n",
        "    return negation_count >= 2\n",
        "\n",
        "def extract_ground_truth(record, record_index):\n",
        "    gt_neg = set()\n",
        "    gt_unc = set()\n",
        "\n",
        "    text = record.get(\"data\", {}).get(\"text\", \"\")\n",
        "    entries = record.get(\"annotations\") or record.get(\"predictions\") or []\n",
        "\n",
        "    for ann in entries:\n",
        "        for res in ann.get(\"result\", []):\n",
        "            labels = res.get(\"value\", {}).get(\"labels\", [])\n",
        "            start = res.get(\"value\", {}).get(\"start\", None)\n",
        "            end = res.get(\"value\", {}).get(\"end\", None)\n",
        "\n",
        "            # Extraer el texto original usando índices, si no hay campo \"text\"\n",
        "            if start is not None and end is not None and start < end and end <= len(text):\n",
        "                text_val = text[start:end].lower().strip()\n",
        "                if not text_val:\n",
        "                    print(f\" Empty span extracted in record {record_index}: start={start}, end={end}\")\n",
        "                    continue\n",
        "\n",
        "                if \"NEG\" in labels:\n",
        "                    gt_neg.add((record_index, start, end, text_val))\n",
        "                if \"UNC\" in labels:\n",
        "                    gt_unc.add((record_index, start, end, text_val))\n",
        "\n",
        "    return gt_neg, gt_unc\n",
        "\n",
        "\n",
        "\n",
        "def detect_medical_suffix(word):\n",
        "    \"\"\"Return True if the word ends with any of the defined medical suffixes.\"\"\"\n",
        "    for suffix in MEDICAL_SUFFIXES:\n",
        "        if word.endswith(suffix):\n",
        "            return True\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "kGKDuNhD3zch"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main analysis Function**  \n",
        "This function willbring everything together. It will go through each sentence, looking for negation or uncertainty cues, and analyzing the surrounding context to detect relevant medical terms either from the UMLS list or by matching medical suffixes.\n",
        "\n",
        "It also groups detections by context, flags double negation when found, and keeps track of everything for evaluation later on.\n"
      ],
      "metadata": {
        "id": "MRhITvQY4OzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def analyze_medical_context(text, nlp):\n",
        "    \"\"\"\n",
        "    Uses PhraseMatchers to detect negation and uncertainty cues in context windows.\n",
        "    Expands with bigram medical term detection and increased context window.\n",
        "    \"\"\"\n",
        "    normalized_text = normalize_text(text)\n",
        "    doc = nlp(normalized_text)\n",
        "    results = {\n",
        "        \"negated_terms_grouped\": [],\n",
        "        \"uncertain_terms_grouped\": [],\n",
        "        \"double_negated_terms\": [],\n",
        "        \"negation_cues_used\": Counter(),\n",
        "        \"uncertainty_cues_used\": Counter(),\n",
        "        \"medical_terms_found\": Counter(),\n",
        "        \"medical_suffix_terms\": []\n",
        "    }\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        tokens = list(sent)\n",
        "        sent_tokens_text = [token.text.lower() for token in tokens]\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            cue_found = False\n",
        "\n",
        "            # NEGATION MATCHER\n",
        "            for cue in NEGATION_WORDS:\n",
        "                    cue_tokens = cue.split()\n",
        "                    if i + len(cue_tokens) <= len(tokens):\n",
        "                        candidate = \" \".join([tokens[j].text.lower() for j in range(i, i+len(cue_tokens))])\n",
        "                        if candidate == cue:\n",
        "                            window_start = max(0, i - 5)\n",
        "                            window_end = min(len(tokens), i + len(cue_tokens) + 5)\n",
        "                            context = \" \".join([tokens[j].text for j in range(window_start, window_end)])\n",
        "                            window_tokens = sent_tokens_text[window_start:window_end]\n",
        "                            detections = []\n",
        "                            for j in range(window_start, window_end):\n",
        "                                term_candidate = tokens[j].text.lower()\n",
        "                                if (term_candidate in [t.lower() for t in UMLS_MEDICAL_TERMS]) or detect_medical_suffix(term_candidate):\n",
        "                                    detection = {\n",
        "                                        \"term\": term_candidate,\n",
        "                                        \"detected_text\": tokens[j].text,\n",
        "                                        \"start\": tokens[j].idx,\n",
        "                                        \"end\": tokens[j].idx + len(tokens[j].text)\n",
        "                                    }\n",
        "                                    detections.append(detection)\n",
        "                                    results[\"medical_terms_found\"][term_candidate] += 1\n",
        "                                    if detect_medical_suffix(term_candidate) and term_candidate not in [t.lower() for t in UMLS_MEDICAL_TERMS]:\n",
        "                                        results[\"medical_suffix_terms\"].append(detection)\n",
        "                            if detections:\n",
        "                                grouped_detection = {\n",
        "                                    \"cue\": cue,\n",
        "                                    \"context\": context,\n",
        "                                    \"window_start\": tokens[window_start].idx,\n",
        "                                    \"window_end\": tokens[window_end-1].idx + len(tokens[window_end-1].text),\n",
        "                                    \"detections\": detections\n",
        "                                }\n",
        "                                if is_double_negation(window_tokens):\n",
        "                                    results[\"double_negated_terms\"].append(grouped_detection)\n",
        "                                else:\n",
        "                                    results[\"negated_terms_grouped\"].append(grouped_detection)\n",
        "                                results[\"negation_cues_used\"].update([cue])\n",
        "                            i += len(cue_tokens)\n",
        "                            cue_found = True\n",
        "                            break\n",
        "            if cue_found:\n",
        "              continue\n",
        "\n",
        "            # UNCERTAINTY MATCHER\n",
        "            for cue in UNCERTAINTY_WORDS:\n",
        "                cue_tokens = cue.split()\n",
        "                if i + len(cue_tokens) <= len(tokens):\n",
        "                    candidate = \" \".join([tokens[j].text.lower() for j in range(i, i+len(cue_tokens))])\n",
        "                    if candidate == cue:\n",
        "                        window_start = max(0, i - 5)\n",
        "                        window_end = min(len(tokens), i + len(cue_tokens) + 5)\n",
        "                        context = \" \".join([tokens[j].text for j in range(window_start, window_end)])\n",
        "                        window_tokens = sent_tokens_text[window_start:window_end]\n",
        "                        detections = []\n",
        "                        for j in range(window_start, window_end):\n",
        "                            term_candidate = tokens[j].text.lower()\n",
        "                            if (term_candidate in [t.lower() for t in UMLS_MEDICAL_TERMS]) or detect_medical_suffix(term_candidate):\n",
        "                                detection = {\n",
        "                                    \"term\": term_candidate,\n",
        "                                    \"detected_text\": tokens[j].text,\n",
        "                                    \"start\": tokens[j].idx,\n",
        "                                    \"end\": tokens[j].idx + len(tokens[j].text)\n",
        "                                }\n",
        "                                detections.append(detection)\n",
        "                                results[\"medical_terms_found\"][term_candidate] += 1\n",
        "                        if detections:\n",
        "                            grouped_detection = {\n",
        "                                \"cue\": cue,\n",
        "                                \"context\": context,\n",
        "                                \"window_start\": tokens[window_start].idx,\n",
        "                                \"window_end\": tokens[window_end-1].idx + len(tokens[window_end-1].text),\n",
        "                                \"detections\": detections\n",
        "                            }\n",
        "                            results[\"uncertainty_cues_used\"].update([cue])\n",
        "                            results[\"uncertain_terms_grouped\"].append(grouped_detection)\n",
        "                        i += len(cue_tokens)\n",
        "                        cue_found = True\n",
        "                        break\n",
        "            if not cue_found:\n",
        "                i += 1\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "y6EsTrsw4UxA"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation metrics:**\n",
        "\n",
        "We will now impkement a function to calculate precision, recall, and F1 score. Which are basic but essential metrics to evaluate how well our system detects negation and uncertainty compared to the annotated ground truth"
      ],
      "metadata": {
        "id": "B4LuSmKP4ane"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(predicted, ground_truth):\n",
        "    tp = len(predicted & ground_truth)\n",
        "    fp = len(predicted - ground_truth)\n",
        "    fn = len(ground_truth - predicted)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    jaccard = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
        "    return precision, recall, f1, jaccard\n"
      ],
      "metadata": {
        "id": "FqABvcgr4avk"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final execution  evaluation and export**  \n",
        "Great! Now we that we have everything we need. Lets implement the main function that will run the entire pipeline on the test set. It loads the model and data, processes each record, applies the detection logic, and updates statistics and evaluation metrics for negation and uncertainty.\n",
        "\n",
        "It also prints out cue frequencies, example detections, and performance results (precision, recall, F1, and Jaccard accuracy).  \n",
        "At the end, it saves the structured output to a JSON file and makes it available for download.\n"
      ],
      "metadata": {
        "id": "iKLCG2zV5bQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    import time\n",
        "    from google.colab import files\n",
        "\n",
        "    print(\"Loading SpaCy model...\")\n",
        "    nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "    print(\"Please upload your JSON file...\")\n",
        "    uploaded = files.upload()\n",
        "    filename = next(iter(uploaded))\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    all_predicted_neg = set()\n",
        "    all_predicted_unc = set()\n",
        "    all_ground_truth_neg = set()\n",
        "    all_ground_truth_unc = set()\n",
        "\n",
        "    total_negation_counter = Counter()\n",
        "    total_uncertainty_counter = Counter()\n",
        "    total_medical_counter = Counter()\n",
        "    negated_examples = []\n",
        "    uncertain_examples = []\n",
        "    records_with_neg = []\n",
        "    records_with_unc = []\n",
        "    output_data = []\n",
        "\n",
        "    start_time = time.time()\n",
        "    print(f\"Processing {len(data)} records...\")\n",
        "    for i, record in enumerate(data):\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Processing record {i+1}/{len(data)}...\")\n",
        "        text = record.get(\"data\", {}).get(\"text\", \"\")\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        results = analyze_medical_context(text, nlp)\n",
        "\n",
        "        total_negation_counter.update(results[\"negation_cues_used\"])\n",
        "        total_uncertainty_counter.update(results[\"uncertainty_cues_used\"])\n",
        "        total_medical_counter.update(results[\"medical_terms_found\"])\n",
        "\n",
        "        for group in results.get(\"negated_terms_grouped\", []):\n",
        "            cue_text = normalize_for_eval(group[\"cue\"])\n",
        "            all_predicted_neg.add((i, cue_text))\n",
        "\n",
        "        for group in results.get(\"uncertain_terms_grouped\", []):\n",
        "            cue_text = normalize_for_eval(group[\"cue\"])\n",
        "            all_predicted_unc.add((i, cue_text))\n",
        "\n",
        "        # Extract GT\n",
        "        gt_neg, gt_unc = extract_ground_truth(record, i)\n",
        "        for (_, _, _, cue_text) in gt_neg:\n",
        "            all_ground_truth_neg.add((i, normalize_for_eval(cue_text)))\n",
        "        for (_, _, _, cue_text) in gt_unc:\n",
        "            all_ground_truth_unc.add((i, normalize_for_eval(cue_text)))\n",
        "\n",
        "        # Ejemplos para visualización\n",
        "        if results.get(\"negated_terms_grouped\"):\n",
        "            records_with_neg.append(i)\n",
        "            for group in results[\"negated_terms_grouped\"]:\n",
        "                terms_info = \"; \".join([f\"{d['term']} (detected: {d['detected_text']})\" for d in group[\"detections\"]])\n",
        "                if len(negated_examples) < 20:\n",
        "                    negated_examples.append(\n",
        "                        f\"Record {i}: Context: '{group['context']}' (cue: {group['cue']}, terms: {terms_info}, window: {group['window_start']}-{group['window_end']})\"\n",
        "                    )\n",
        "\n",
        "        if results.get(\"uncertain_terms_grouped\"):\n",
        "            records_with_unc.append(i)\n",
        "            for group in results[\"uncertain_terms_grouped\"]:\n",
        "                terms_info = \"; \".join([f\"{d['term']} (detected: {d['detected_text']})\" for d in group[\"detections\"]])\n",
        "                if len(uncertain_examples) < 20:\n",
        "                    uncertain_examples.append(\n",
        "                        f\"Record {i}: Context: '{group['context']}' (cue: {group['cue']}, terms: {terms_info}, window: {group['window_start']}-{group['window_end']})\"\n",
        "                    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    processing_time = end_time - start_time\n",
        "\n",
        "    print(\"\\nGround truth samples:\")\n",
        "    for item in list(all_ground_truth_neg)[:5]:\n",
        "        print(\"GT:\", item)\n",
        "\n",
        "    print(\"\\nPredicted samples:\")\n",
        "    for item in list(all_predicted_neg)[:5]:\n",
        "        print(\"PRED:\", item)\n",
        "\n",
        "    intersection = all_predicted_neg & all_ground_truth_neg\n",
        "    print(\"\\nINTERSECTION:\")\n",
        "    for match in list(intersection)[:5]:\n",
        "        print(\"MATCH:\", match)\n",
        "\n",
        "    prec_neg, rec_neg, f1_neg, acc_neg = compute_metrics(all_predicted_neg, all_ground_truth_neg)\n",
        "    prec_unc, rec_unc, f1_unc, acc_unc = compute_metrics(all_predicted_unc, all_ground_truth_unc)\n",
        "\n",
        "    combined_pred = all_predicted_neg.union(all_predicted_unc)\n",
        "    combined_gt = all_ground_truth_neg.union(all_ground_truth_unc)\n",
        "    prec_comb, rec_comb, f1_comb, acc_comb = compute_metrics(combined_pred, combined_gt)\n",
        "\n",
        "    print(\"\\n=== STATISTICS ===\")\n",
        "    print(\"\\nNegation Cue Frequencies (affecting medical terms):\")\n",
        "    for word, freq in total_negation_counter.most_common():\n",
        "        print(f\"{word}: {freq}\")\n",
        "\n",
        "    print(\"\\nUncertainty Cue Frequencies (affecting medical terms):\")\n",
        "    for word, freq in total_uncertainty_counter.most_common():\n",
        "        print(f\"{word}: {freq}\")\n",
        "\n",
        "    print(\"\\nMedical Terms Frequencies:\")\n",
        "    for word, freq in total_medical_counter.most_common():\n",
        "        print(f\"{word}: {freq}\")\n",
        "\n",
        "    print(\"\\n=== EXAMPLES OF NEGATED MEDICAL TERMS ===\")\n",
        "    for example in negated_examples:\n",
        "        print(example)\n",
        "\n",
        "    print(\"\\n=== EXAMPLES OF UNCERTAIN MEDICAL TERMS ===\")\n",
        "    for example in uncertain_examples:\n",
        "        print(example)\n",
        "\n",
        "    print(\"\\n=== TOTAL DETECTED CONTEXTS ===\")\n",
        "    print(f\"Total detected negated contexts: {len(all_predicted_neg)}\")\n",
        "    print(f\"Total detected uncertain contexts: {len(all_predicted_unc)}\")\n",
        "    print(f\"Total combined detected contexts: {len(combined_pred)}\")\n",
        "\n",
        "    print(\"\\n=== RECORDS WITH DETECTIONS ===\")\n",
        "    print(f\"Records with negation detections: {records_with_neg}\")\n",
        "    print(f\"Records with uncertainty detections: {records_with_unc}\")\n",
        "\n",
        "    print(\"\\n=== EFFICIENCY ===\")\n",
        "    print(f\"Total processing time: {processing_time:.2f} seconds\")\n",
        "\n",
        "    print(\"\\n=== EVALUATION METRICS ===\")\n",
        "    print(\"\\nNegation Metrics:\")\n",
        "    print(f\"Precision: {prec_neg:.2f}, Recall: {rec_neg:.2f}, F1: {f1_neg:.2f}, Accuracy (Jaccard): {acc_neg:.2f}\")\n",
        "    print(f\"Matches found using cue containment: {len(intersection)}\")\n",
        "\n",
        "    print(\"\\nUncertainty Metrics:\")\n",
        "    print(f\"Precision: {prec_unc:.2f}, Recall: {rec_unc:.2f}, F1: {f1_unc:.2f}, Accuracy (Jaccard): {acc_unc:.2f}\")\n",
        "\n",
        "    output_filename = \"output.json\"\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as outfile:\n",
        "        json.dump(output_data, outfile, ensure_ascii=False, indent=2)\n",
        "    print(f\"\\nOutput file '{output_filename}' generated.\")\n",
        "    files.download(output_filename)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "oXK2hlkOg8ry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "573683ef-aa73-4d61-b748-aab9f83ab275"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SpaCy model...\n",
            "Please upload your JSON file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-294b00c0-712a-4045-bb86-1cc5a6de2310\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-294b00c0-712a-4045-bb86-1cc5a6de2310\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving negacio_test_v2024.json to negacio_test_v2024 (18).json\n",
            "Processing 64 records...\n",
            "Processing record 1/64...\n",
            "Processing record 11/64...\n",
            "Processing record 21/64...\n",
            "Processing record 31/64...\n",
            "Processing record 41/64...\n",
            "Processing record 51/64...\n",
            "Processing record 61/64...\n",
            "\n",
            "Ground truth samples:\n",
            "GT: (20, 'no')\n",
            "GT: (31, 'no')\n",
            "GT: (21, 'negativo')\n",
            "GT: (57, 'afebril,')\n",
            "GT: (54, 'niega')\n",
            "\n",
            "Predicted samples:\n",
            "PRED: (20, 'no')\n",
            "PRED: (31, 'no')\n",
            "PRED: (53, 'no')\n",
            "PRED: (57, 'sin')\n",
            "PRED: (55, 'no')\n",
            "\n",
            "INTERSECTION:\n",
            "MATCH: (20, 'no')\n",
            "MATCH: (53, 'no')\n",
            "MATCH: (31, 'no')\n",
            "MATCH: (55, 'no')\n",
            "MATCH: (63, 'sin')\n",
            "\n",
            "=== STATISTICS ===\n",
            "\n",
            "Negation Cue Frequencies (affecting medical terms):\n",
            "no: 74\n",
            "sin: 46\n",
            "ni: 19\n",
            "niega: 5\n",
            "negativa: 3\n",
            "ausencia de: 3\n",
            "negativo: 1\n",
            "descarta: 1\n",
            "\n",
            "Uncertainty Cue Frequencies (affecting medical terms):\n",
            "compatible con: 5\n",
            "probable: 5\n",
            "posible: 4\n",
            "probablemente: 4\n",
            "sospecha de: 3\n",
            "aparentemente: 3\n",
            "puede: 1\n",
            "se considera: 1\n",
            "parece: 1\n",
            "sugestivo de: 1\n",
            "aproximadamente: 1\n",
            "\n",
            "Medical Terms Frequencies:\n",
            "aguda: 21\n",
            "estenosis: 13\n",
            "parto: 10\n",
            "insuficiencia: 9\n",
            "ascitis: 7\n",
            "neoplasia: 7\n",
            "fractura: 6\n",
            "coluria: 5\n",
            "isquemia: 5\n",
            "hipertension: 5\n",
            "coledocolitiasis: 5\n",
            "ectasia: 4\n",
            "arterial: 4\n",
            "renal: 4\n",
            "protesis: 3\n",
            "axonal: 3\n",
            "sensitiva: 3\n",
            "membranas: 3\n",
            "hematoma: 3\n",
            "uretra: 3\n",
            "cronica: 3\n",
            "pancreatitis: 3\n",
            "dislipemia: 3\n",
            "bacteriuria: 2\n",
            "leucocituria: 2\n",
            "fibrinolisis: 2\n",
            "pielonefritis: 2\n",
            "renales: 2\n",
            "gastroenteritis: 2\n",
            "colitis: 2\n",
            "colecistectomia: 2\n",
            "laparoscopica: 2\n",
            "cianosis: 2\n",
            "hipoxemia: 2\n",
            "neuropatia: 2\n",
            "displasia: 2\n",
            "polimialgia: 2\n",
            "parcial: 2\n",
            "diverticulosis: 2\n",
            "colecistitis: 2\n",
            "infarto: 2\n",
            "ictus: 2\n",
            "hiponatremia: 2\n",
            "adenoidectomia: 1\n",
            "uretritis: 1\n",
            "neutropenia: 1\n",
            "viriasis: 1\n",
            "cerebral: 1\n",
            "hipopotasemia: 1\n",
            "atelectasia: 1\n",
            "trombectomia: 1\n",
            "carcinoma: 1\n",
            "macroscopica: 1\n",
            "adenocarcinoma: 1\n",
            "quistes: 1\n",
            "itsmocele: 1\n",
            "noitsmocele: 1\n",
            "bacteriemia: 1\n",
            "cistoscopia: 1\n",
            "apendicectomia: 1\n",
            "reestenosis: 1\n",
            "nefritis: 1\n",
            "angioma: 1\n",
            "laminectomia: 1\n",
            "mioma: 1\n",
            "litiasis: 1\n",
            "rotura: 1\n",
            "hematomaseroma: 1\n",
            "hipernefroma: 1\n",
            "laparoscopia: 1\n",
            "-antigenuria: 1\n",
            "antigenuria: 1\n",
            "gastrectomia: 1\n",
            "ecocardioscopia: 1\n",
            "ateroma: 1\n",
            "hipokalemia: 1\n",
            "mandibular: 1\n",
            "esofagitis: 1\n",
            "multifactorial: 1\n",
            "piuria: 1\n",
            "encefalopatia: 1\n",
            "ureterorrenoscopia: 1\n",
            "toma: 1\n",
            "pneumonitis: 1\n",
            "anuria: 1\n",
            "oliguria: 1\n",
            "venoclisis: 1\n",
            "anemia: 1\n",
            "\n",
            "=== EXAMPLES OF NEGATED MEDICAL TERMS ===\n",
            "Record 0: Context: ': protesis mamaria , adenoidectomia niega habitos toxicos medicacio habitual anafranil25' (cue: niega, terms: protesis (detected: protesis); adenoidectomia (detected: adenoidectomia), window: 392-479)\n",
            "Record 1: Context: ', hbcac negatiu , vhc negativa -antecedentes its : uretritis gonococica' (cue: negativa, terms: uretritis (detected: uretritis), window: 714-783)\n",
            "Record 1: Context: 'orina muestra escasa bacteriuria , sin leucocituria .' (cue: sin, terms: bacteriuria (detected: bacteriuria); leucocituria (detected: leucocituria), window: 5984-6035)\n",
            "Record 1: Context: 'orientacio diagnostica b34.9 infeccio virica no especificada neutropenia autolimitada fiebre de' (cue: no, terms: neutropenia (detected: neutropenia), window: 7100-7195)\n",
            "Record 2: Context: 'sin signos de insuficiencia cardiaca derecha' (cue: sin, terms: insuficiencia (detected: insuficiencia), window: 2068-2112)\n",
            "Record 2: Context: 'sin signos de insuficiencia cardiaca derecha' (cue: sin, terms: insuficiencia (detected: insuficiencia), window: 3245-3289)\n",
            "Record 2: Context: 'sensitiva mes expresiva a eess no se observan claros signos de' (cue: no, terms: sensitiva (detected: sensitiva), window: 7617-7679)\n",
            "Record 2: Context: 'rm cerebral y medular que no muestra hallazgos significativos a destacar' (cue: no, terms: cerebral (detected: cerebral), window: 7822-7894)\n",
            "Record 2: Context: 'analiticamente hipopotasemia leve sin otros hallazgos a destacar .' (cue: sin, terms: hipopotasemia (detected: hipopotasemia), window: 8371-8436)\n",
            "Record 2: Context: ', sugiere origen inmunomediado ; sin poderse definir un patron axonal' (cue: sin, terms: axonal (detected: axonal), window: 9494-9562)\n",
            "Record 3: Context: '-rx tobillo : no se aprecian signos de fractura' (cue: no, terms: fractura (detected: fractura), window: 3170-3216)\n",
            "Record 3: Context: 'riesgo intermedio-alto , aunque inicialmente no se considera para fibrinolisis o' (cue: no, terms: fibrinolisis (detected: fibrinolisis), window: 7857-7936)\n",
            "Record 4: Context: 'de citorreduccion : hallazgos ; no ascitis .' (cue: no, terms: ascitis (detected: ascitis), window: 2626-2666)\n",
            "Record 4: Context: 'ausencia de enfermedad macroscopica residual tr0.se administra' (cue: ausencia de, terms: macroscopica (detected: macroscopica), window: 2837-2899)\n",
            "Record 4: Context: 'sin signos de focalidad neurologica aguda' (cue: sin, terms: aguda (detected: aguda), window: 8335-8376)\n",
            "Record 6: Context: 'episodio de pielonefritis obstructiva derecha sin incidencias .' (cue: sin, terms: pielonefritis (detected: pielonefritis), window: 838-900)\n",
            "Record 6: Context: 'sugestivas de quistes renales , no neoplasicas ; cateter 2j derecho' (cue: no, terms: quistes (detected: quistes); renales (detected: renales), window: 1102-1167)\n",
            "Record 7: Context: 'protesis , por lo que no es encesario ningun procedimiento .' (cue: no, terms: protesis (detected: protesis), window: 388-446)\n",
            "Record 8: Context: 'area cicatriu de 2,5 mm no itsmocele , cervix de 34' (cue: no, terms: itsmocele (detected: itsmocele), window: 929-979)\n",
            "Record 8: Context: 'de 1.8 mm lt;p5 noitsmocele ni nicho doppler normal - 20.12.17' (cue: ni, terms: noitsmocele (detected: noitsmocele), window: 1058-1119)\n",
            "\n",
            "=== EXAMPLES OF UNCERTAIN MEDICAL TERMS ===\n",
            "Record 1: Context: 'fiebre de origen desconocido , posible viriasis .' (cue: posible, terms: viriasis (detected: viriasis), window: 7186-7233)\n",
            "Record 2: Context: 'no se puede definir un patron axonal o' (cue: puede, terms: axonal (detected: axonal), window: 5817-5855)\n",
            "Record 3: Context: 'lineal en base de lii compatible con atelectasia subsegmentaria .' (cue: compatible con, terms: atelectasia (detected: atelectasia), window: 4257-4321)\n",
            "Record 3: Context: 'intermedio-alto , aunque inicialmente no se considera para fibrinolisis o trombectomia .' (cue: se considera, terms: fibrinolisis (detected: fibrinolisis); trombectomia (detected: trombectomia), window: 7864-7950)\n",
            "Record 4: Context: '1º diagnostico : ante la sospecha de carcinoma de ovario se realiza' (cue: sospecha de, terms: carcinoma (detected: carcinoma), window: 660-726)\n",
            "Record 4: Context: ': positivo para celulas malignas compatible con metastasis de adenocarcinoma de origen' (cue: compatible con, terms: adenocarcinoma (detected: adenocarcinoma), window: 5189-5275)\n",
            "Record 10: Context: 'placenta y membranas aparentemente integras .' (cue: aparentemente, terms: membranas (detected: membranas), window: 2646-2690)\n",
            "Record 16: Context: 'de uretra posterior , que parece retener contraste de forma continuada' (cue: parece, terms: uretra (detected: uretra), window: 2245-2314)\n",
            "Record 21: Context: 'hiperecogenica , riñon derecho con posible area de nefritis .' (cue: posible, terms: nefritis (detected: nefritis), window: 3050-3109)\n",
            "Record 21: Context: 'homogeneo e imagen hiperecogenica nodular compatible con pequeño angioma hepatico .' (cue: compatible con, terms: angioma (detected: angioma), window: 3197-3279)\n",
            "Record 22: Context: 'aparentemente sin focalidad aguda .' (cue: aparentemente, terms: aguda (detected: aguda), window: 4487-4521)\n",
            "Record 30: Context: 'cronica de etiologia no concluyente probable nefroangioesclerosis diagnosticada hace años ,' (cue: probable, terms: cronica (detected: cronica), window: 870-960)\n",
            "Record 30: Context: '-ingreso en marzo 2018 por posible pancreatitis aguda no cofirmada .' (cue: posible, terms: pancreatitis (detected: pancreatitis); aguda (detected: aguda), window: 2417-2484)\n",
            "Record 36: Context: 'con dionando ectasia izquierda y posible extension hacia cuello vesical con' (cue: posible, terms: ectasia (detected: ectasia), window: 2501-2576)\n",
            "Record 42: Context: 'probable trombosis parcial de confluente esplenoportal' (cue: probable, terms: parcial (detected: parcial), window: 1935-1989)\n",
            "Record 44: Context: 'izquierda de 19 mm patron sugestivo de estenosis pieloureteral .  ' (cue: sugestivo de, terms: estenosis (detected: estenosis), window: 570-634)\n",
            "Record 45: Context: 'compatible con hipernefroma .' (cue: compatible con, terms: hipernefroma (detected: hipernefroma), window: 901-929)\n",
            "Record 47: Context: 'pequeño infarto cronico que probablemente corresponde al territorio perforante de' (cue: probablemente, terms: infarto (detected: infarto), window: 8392-8473)\n",
            "Record 47: Context: 'deterioro de la funcion renal probablemente prerenal en contexto de deshidratacion' (cue: probablemente, terms: renal (detected: renal), window: 9161-9243)\n",
            "Record 49: Context: '12 horas de la intervencion aproximadamente se detectan signos de hematoma' (cue: aproximadamente, terms: hematoma (detected: hematoma), window: 810-884)\n",
            "\n",
            "=== TOTAL DETECTED CONTEXTS ===\n",
            "Total detected negated contexts: 73\n",
            "Total detected uncertain contexts: 28\n",
            "Total combined detected contexts: 101\n",
            "\n",
            "=== RECORDS WITH DETECTIONS ===\n",
            "Records with negation detections: [0, 1, 2, 3, 4, 6, 7, 8, 14, 15, 16, 17, 19, 20, 21, 22, 24, 25, 26, 29, 30, 31, 32, 33, 34, 35, 38, 40, 41, 43, 44, 45, 46, 47, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
            "Records with uncertainty detections: [1, 2, 3, 4, 10, 16, 21, 22, 30, 36, 42, 44, 45, 47, 49, 50, 53, 54, 55, 61, 62, 63]\n",
            "\n",
            "=== EFFICIENCY ===\n",
            "Total processing time: 17.36 seconds\n",
            "\n",
            "=== EVALUATION METRICS ===\n",
            "\n",
            "Negation Metrics:\n",
            "Precision: 0.96, Recall: 0.24, F1: 0.38, Accuracy (Jaccard): 0.23\n",
            "Matches found using cue containment: 70\n",
            "\n",
            "Uncertainty Metrics:\n",
            "Precision: 0.86, Recall: 0.21, F1: 0.34, Accuracy (Jaccard): 0.20\n",
            "\n",
            "Output file 'output.json' generated.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f7a43670-15a6-4cfe-8974-8864786d5e58\", \"output.json\", 2)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWDrlxRrOAHI"
      },
      "execution_count": 77,
      "outputs": []
    }
  ]
}