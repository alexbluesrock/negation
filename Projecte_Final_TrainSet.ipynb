{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Fundamentals of Natural Language Processing**"
      ],
      "metadata": {
        "id": "ckSgdIBgn1bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Negation and Uncertainty Detection Project#"
      ],
      "metadata": {
        "id": "DAIBC-VboGAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code 1\n",
        "\n"
      ],
      "metadata": {
        "id": "TvoEH9xi0CsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing Required Libraries:**\n",
        "\n",
        "We begin by installing the necessary NLP libraries, including spaCy and its Spanish language model, which will be used later for tokenization and syntactic analysis."
      ],
      "metadata": {
        "id": "81XETNAZpBo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (execute only if not already installed)\n",
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koK_X3oBjDm0",
        "outputId": "b766548b-34fb-4037-e424-c774fbe7def1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing libraries for text processing, NLP, and file handling in Colab**"
      ],
      "metadata": {
        "id": "watr8LeNpp7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "import spacy\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "dYBDaFU4ppYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting any analysis, we define a function to normalize the clinical text, preparing the data for further processing\n"
      ],
      "metadata": {
        "id": "Z0t53aKhqDrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to normalize text\n",
        "def normalize_text(text):\n",
        "    \"\"\"Normalize medical text with Spanish/Catalan character support\"\"\"\n",
        "    # Convert to lowercase first, then normalize\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove sensitive patient identifiers\n",
        "    text = re.sub(r'\\*+', '', text)\n",
        "\n",
        "    # Remove unnecessary punctuation (but keep medical-relevant ones)\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?-àáèéìíòóùúüñç]', '', text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "uge9Kju4qD9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define the key words and phrases that indicate negation, uncertainty, and relevant medical concepts in Spanish and Catalan; that will guide our detection system"
      ],
      "metadata": {
        "id": "ZycUdQ77qdqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  # Define negation, uncertainty and UMLS medical terms\n",
        "\n",
        "NEGATION_WORDS = [\n",
        "      # Common negation words\n",
        "      \"no\", \"sin\", \"ausencia de\", \"descarta\", \"descartado\", \"excluye\", \"excluido\", \"niega\", \"negado\",\n",
        "      \"negativa\", \"negación\", \"ningún\", \"ninguna\", \"ninguno\", \"imposible\", \"inhallable\", \"carece de\", \"nunca\",\n",
        "      \"jamás\", \"tampoco\", \"ni\", \"nada\", \"negativo\", \"mai\",\n",
        "\n",
        "      # Medical-specific negation in Spanish\n",
        "      \"sin evidencia de\", \"no se observa\", \"no presenta\", \"no muestra\", \"no evidencia\", \"no compatible con\",\n",
        "      \"no concluyente\", \"no parece\", \"no se detecta\", \"sin signos de\", \"sin síntomas de\", \"sin indicios de\",\n",
        "      \"sin hallazgos de\", \"sin pruebas de\", \"sin rastro de\", \"ausente\", \"no encontrado\", \"sin cambios\",\n",
        "      \"no se aprecian\", \"no se ven\", \"descartando\", \"descartable\", \"no hay evidencia de\", \"no hay indicación de\",\n",
        "      \"libre de\", \"exento de\", \"sin manifestaciones de\", \"se excluye\", \"queda descartado\", \"ninguna evidencia de\",\n",
        "      \"ningún signo de\", \"sin afección\", \"no identificado\", \"negado por el paciente\", \"negado clínicamente\",\n",
        "      \"sin enfermedad\", \"sin afectación\", \"no afectado\", \"no positivo\", \"resultado negativo\",\n",
        "      \"resultado no reactivo\", \"resultado no positivo\",\n",
        "\n",
        "      # Medical-specific negation in Catalan\n",
        "      \"sense\", \"no es detecta\", \"no es veu\", \"no hi ha\", \"no presenta\", \"sense indicis de\", \"sense evidència de\",\n",
        "      \"sense senyals de\", \"sense rastre de\", \"sense afectació\", \"sense afecció\", \"no concloent\", \"sense canvis\",\n",
        "      \"sense resultats\", \"sense manifestacions de\", \"no s'observa\", \"no s'aprecia\", \"sense presència de\",\n",
        "      \"no compatible amb\", \"no és visible\", \"sense símptomes\", \"no diagnosticat\", \"sense senyals clars\",\n",
        "      \"diagnòstic negatiu\"\n",
        "  ]\n",
        "\n",
        "UNCERTAINTY_WORDS = [\n",
        "      # Common & medical uncertainty words in Spanish\n",
        "      \"posible\", \"quizás\", \"podría\", \"sospecha de\", \"considera\", \"probable\", \"aparentemente\", \"puede\", \"posiblemente\",\n",
        "      \"parece\", \"se considera\", \"indeterminado\", \"probabilidad de\", \"no concluyente\", \"eventual\", \"en estudio\",\n",
        "      \"pendiente de evaluación\", \"sugestivo de\", \"sugiere\", \"indica que\", \"se sospecha de\", \"podría indicar\",\n",
        "      \"dudoso\", \"no definido\", \"no específico\", \"no determinado\", \"valor incierto\", \"no claro\", \"no seguro\",\n",
        "      \"compatible con\", \"aparenta ser\", \"tendría que evaluarse\", \"a determinar\", \"probabilidad baja de\",\n",
        "      \"probabilidad alta de\", \"sin certeza\", \"hipotético\", \"hipotéticamente\", \"a confirmar\", \"falta de certeza\",\n",
        "      \"en posible relación con\", \"estaría asociado\", \"aparentemente relacionado con\", \"se intuye\", \"se deduce que\",\n",
        "      \"en consideración\", \"posible\", \"probablemente\", \"tal vez\", \"aproximadamente\", \"probable\",\n",
        "\n",
        "      # Common & medical uncertainty in Catalan\n",
        "      \"possible\", \"potser\", \"podria\", \"sospita de\", \"es considera\", \"probable\", \"aparentment\", \"pot ser\",\n",
        "      \"possiblement\", \"sembla\", \"es sospita de\", \"és indeterminat\", \"probabilitat de\", \"no concloent\", \"eventual\",\n",
        "      \"en estudi\", \"pendent d'avaluació\", \"suggerent de\", \"suggerix\", \"indica que\", \"dubtós\", \"no definit\",\n",
        "      \"no específic\", \"no determinat\", \"valor incert\", \"no clar\", \"no segur\", \"aparentment relacionat amb\",\n",
        "      \"es dedueix que\", \"en consideració\"\n",
        "  ]\n",
        "\n",
        "UMLS_MEDICAL_TERMS = [\n",
        "      \"uretrotomia\", \"interna\", \"cistoscopia\", \"estenosis\", \"uretra\", \"cronica\", \"diverticulosis\", \"insuficiencia\",\n",
        "      \"renal\", \"colelitiasis\", \"bloqueo\", \"auriculoventricular\", \"primer grado\", \"segundo grado\", \"hipertension\",\n",
        "      \"arterial\", \"protesis\", \"cadera\", \"cordectomia\", \"herniorrafia\", \"parto\", \"eutocico\", \"rotura\", \"membranas\",\n",
        "      \"prematuro\", \"episiotomia\", \"lactancia materna\", \"apendicectomia\", \"laparoscopica\", \"gastroenteritis\", \"aguda\",\n",
        "      \"nefrectomia\", \"parcial\", \"angiomiolipoma\", \"quistes\", \"renales\", \"fractura\", \"mandibular\", \"ictus\", \"infarto\",\n",
        "      \"isquemico\", \"trombectomia\", \"cerebral\", \"fibrinolisis\", \"endovenosa\", \"colangitis\", \"microcirugia\",\n",
        "      \"endolaringea\", \"polineuropatia\", \"sensitiva\", \"axonal\", \"neuropatia\", \"multifactorial\", \"mielopatia\", \"déficit\"\n",
        "  ]"
      ],
      "metadata": {
        "id": "mxfS-fpqqdyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we add a function to detect double negation, which helps avoid false positives when multiple negation cues appear together:\n"
      ],
      "metadata": {
        "id": "_ArJK3k_q-qR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to detect double negation in a window around a medical term\n",
        "def is_double_negation(tokens):\n",
        "    \"\"\"Check if a sequence of tokens contains double negation\"\"\"\n",
        "    # Simple cues that would indicate negation (a subset of the full NEGATION_WORDS)\n",
        "    simple_negation_cues = {\"no\", \"sin\", \"nunca\", \"jamás\", \"ningún\", \"ninguna\", \"nadie\", \"ninguno\", \"negado\", \"niega\"}\n",
        "    negation_count = sum(1 for token in tokens if token.lower() in simple_negation_cues)\n",
        "    return negation_count >= 2"
      ],
      "metadata": {
        "id": "aPMP6h83q-zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good! Now that the basics are set up, let’s add a function to analyze medical terms in context to check whether they appear with negation, uncertainty, or double negation cues.\n"
      ],
      "metadata": {
        "id": "fz7hThANrQwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to process a text and find medical terms with their negation/uncertainty context\n",
        "def analyze_medical_context(text, nlp):\n",
        "    # Normalize the text\n",
        "    normalized_text = normalize_text(text)\n",
        "\n",
        "    # Process with spaCy\n",
        "    doc = nlp(normalized_text)\n",
        "\n",
        "    results = {\n",
        "        \"negated_terms\": [],\n",
        "        \"uncertain_terms\": [],\n",
        "        \"double_negated_terms\": [],\n",
        "        \"negation_cues_used\": Counter(),\n",
        "        \"uncertainty_cues_used\": Counter(),\n",
        "        \"medical_terms_found\": Counter()\n",
        "    }\n",
        "\n",
        "    # Process each sentence to better handle context boundaries\n",
        "    for sent in doc.sents:\n",
        "        sent_tokens = [token for token in sent]\n",
        "\n",
        "        # For each token in the sentence\n",
        "        for i, token in enumerate(sent_tokens):\n",
        "            token_text = token.text.lower()\n",
        "\n",
        "            # Check if it's a medical term\n",
        "            if token_text in [term.lower() for term in UMLS_MEDICAL_TERMS]:\n",
        "                # Count this medical term\n",
        "                results[\"medical_terms_found\"][token_text] += 1\n",
        "\n",
        "                # Define the context window (5 tokens before and after)\n",
        "                start_idx = max(0, i - 5)\n",
        "                end_idx = min(len(sent_tokens), i + 6)\n",
        "                context_window = sent_tokens[start_idx:end_idx]\n",
        "                context_tokens = [t.text.lower() for t in context_window]\n",
        "                context_text = \" \".join(context_tokens)\n",
        "\n",
        "                # Check for double negation in context\n",
        "                if is_double_negation(context_tokens):\n",
        "                    results[\"double_negated_terms\"].append({\n",
        "                        \"term\": token_text,\n",
        "                        \"context\": context_text\n",
        "                    })\n",
        "                    continue  # Skip further checks for this term if double negation is found\n",
        "\n",
        "                # Check for negation cues in context\n",
        "                negated = False\n",
        "                for neg_cue in NEGATION_WORDS:\n",
        "                    # For multi-word cues, check if they appear in the context\n",
        "                    if \" \" in neg_cue:\n",
        "                        if neg_cue in context_text:\n",
        "                            results[\"negation_cues_used\"][neg_cue] += 1\n",
        "                            negated = True\n",
        "                            break\n",
        "                    # For single-word cues, check if they appear in the context tokens\n",
        "                    elif neg_cue in context_tokens:\n",
        "                        results[\"negation_cues_used\"][neg_cue] += 1\n",
        "                        negated = True\n",
        "                        break\n",
        "\n",
        "                if negated:\n",
        "                    results[\"negated_terms\"].append({\n",
        "                        \"term\": token_text,\n",
        "                        \"context\": context_text\n",
        "                    })\n",
        "                    continue  # If term is negated, don't check for uncertainty\n",
        "\n",
        "                # Check for uncertainty cues in context\n",
        "                uncertain = False\n",
        "                for unc_cue in UNCERTAINTY_WORDS:\n",
        "                    # For multi-word cues, check if they appear in the context\n",
        "                    if \" \" in unc_cue:\n",
        "                        if unc_cue in context_text:\n",
        "                            results[\"uncertainty_cues_used\"][unc_cue] += 1\n",
        "                            uncertain = True\n",
        "                            break\n",
        "                    # For single-word cues, check if they appear in the context tokens\n",
        "                    elif unc_cue in context_tokens:\n",
        "                        results[\"uncertainty_cues_used\"][unc_cue] += 1\n",
        "                        uncertain = True\n",
        "                        break\n",
        "\n",
        "                if uncertain:\n",
        "                    results[\"uncertain_terms\"].append({\n",
        "                        \"term\": token_text,\n",
        "                        \"context\": context_text\n",
        "                    })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "cQwhqLVPrQ45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we will bring everything together in the main function:\n",
        "Load the language model and dataset, run the analysis on each record, and display overall statistics.\n"
      ],
      "metadata": {
        "id": "2wt5zOZOsbDP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M5i5lrTeix0Y",
        "outputId": "04ef158f-f411-4c79-ac8a-a30de44c7055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SpaCy model...\n",
            "Please upload your JSON file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5562231f-3764-41b5-9f0d-7e23f38ba07b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5562231f-3764-41b5-9f0d-7e23f38ba07b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving negacio_train_v2024.json to negacio_train_v2024.json\n",
            "Processing 254 records...\n",
            "Processing record 1/254...\n",
            "Processing record 11/254...\n",
            "Processing record 21/254...\n",
            "Processing record 31/254...\n",
            "Processing record 41/254...\n",
            "Processing record 51/254...\n",
            "Processing record 61/254...\n",
            "Processing record 71/254...\n",
            "Processing record 81/254...\n",
            "Processing record 91/254...\n",
            "Processing record 101/254...\n",
            "Processing record 111/254...\n",
            "Processing record 121/254...\n",
            "Processing record 131/254...\n",
            "Processing record 141/254...\n",
            "Processing record 151/254...\n",
            "Processing record 161/254...\n",
            "Processing record 171/254...\n",
            "Processing record 181/254...\n",
            "Processing record 191/254...\n",
            "Processing record 201/254...\n",
            "Processing record 211/254...\n",
            "Processing record 221/254...\n",
            "Processing record 231/254...\n",
            "Processing record 241/254...\n",
            "Processing record 251/254...\n",
            "\n",
            "=== STATISTICS ===\n",
            "\n",
            "Negation Cue Frequencies (affecting medical terms):\n",
            "sin: 123\n",
            "no: 101\n",
            "ni: 9\n",
            "sense: 7\n",
            "negativa: 3\n",
            "ausencia de: 3\n",
            "negativo: 3\n",
            "libre de: 1\n",
            "no se observa: 1\n",
            "\n",
            "Uncertainty Cue Frequencies (affecting medical terms):\n",
            "probable: 20\n",
            "posible: 9\n",
            "aparentemente: 9\n",
            "probablemente: 9\n",
            "compatible con: 6\n",
            "sospecha de: 5\n",
            "sugiere: 2\n",
            "parece: 2\n",
            "posiblemente: 2\n",
            "sugestivo de: 1\n",
            "possible: 1\n",
            "\n",
            "Medical Terms Frequencies:\n",
            "parto: 250\n",
            "renal: 198\n",
            "aguda: 154\n",
            "cronica: 97\n",
            "arterial: 85\n",
            "fractura: 84\n",
            "hipertension: 75\n",
            "insuficiencia: 65\n",
            "interna: 64\n",
            "parcial: 50\n",
            "ictus: 44\n",
            "estenosis: 43\n",
            "protesis: 41\n",
            "cerebral: 41\n",
            "eutocico: 40\n",
            "endovenosa: 36\n",
            "membranas: 33\n",
            "infarto: 33\n",
            "apendicectomia: 27\n",
            "nefrectomia: 27\n",
            "laparoscopica: 26\n",
            "cadera: 21\n",
            "episiotomia: 20\n",
            "rotura: 19\n",
            "renales: 18\n",
            "cistoscopia: 16\n",
            "isquemico: 16\n",
            "bloqueo: 14\n",
            "uretra: 14\n",
            "quistes: 12\n",
            "prematuro: 12\n",
            "herniorrafia: 11\n",
            "mandibular: 10\n",
            "sensitiva: 9\n",
            "diverticulosis: 8\n",
            "colangitis: 8\n",
            "neuropatia: 7\n",
            "axonal: 7\n",
            "trombectomia: 7\n",
            "colelitiasis: 6\n",
            "multifactorial: 6\n",
            "gastroenteritis: 5\n",
            "cordectomia: 4\n",
            "polineuropatia: 4\n",
            "uretrotomia: 3\n",
            "angiomiolipoma: 3\n",
            "microcirugia: 3\n",
            "endolaringea: 3\n",
            "fibrinolisis: 3\n",
            "mielopatia: 2\n",
            "auriculoventricular: 1\n",
            "\n",
            "=== EXAMPLES OF NEGATED MEDICAL TERMS ===\n",
            "'de hematuria macroscopica se realiza cistoscopia que es negativa para lesiones' (term: cistoscopia)\n",
            "'la uretrografia miccional muestra una uretra prostatica dilatada , sin claras' (term: uretra)\n",
            "'prostatica dilatada , sin claras estenosis focales confirmandose la existencia de' (term: estenosis)\n",
            "'la uretra se detecta gran estenosis que no permite el paso' (term: estenosis)\n",
            "'contraindicacion preoperatoria se realiza uretrotomia interna sin incidencias .' (term: interna)\n",
            "'dhospitalitzacio motiu dingres coleccion post apendicectomia antecedents sin alergias medicamentosas conocidas' (term: apendicectomia)\n",
            "'20h no inicia trabajo de parto espontaneamente bajar a sala de' (term: parto)\n",
            "'se realiza nefrectomia parcial derecha laparoscopica asistida por robot sin incidencias' (term: laparoscopica)\n",
            "'no especificat , excepte pelvis renal procediments nefrectomia parcial robotica profilaxis' (term: renal)\n",
            "'que muestra higado heterogeneo sin hipertension portal ni loes .' (term: hipertension)\n",
            "'dhospitalitzacio motiu dingres trabajo de parto antecedents no alergias medicamentosas conocidas' (term: parto)\n",
            "'informe dalta dhospitalitzacio motiu dingres rotura prematura de membranas antecedents no' (term: rotura)\n",
            "'motiu dingres rotura prematura de membranas antecedents no alergias medicamentosas conocidas' (term: membranas)\n",
            "'12 lopd orientacio diagnostica s02.609a fractura de mandibula no especificada ,' (term: fractura)\n",
            "'angiomiolipoma renal derecho sin cambios .' (term: angiomiolipoma)\n",
            "'angiomiolipoma renal derecho sin cambios .' (term: renal)\n",
            "'se sutura episiotomia media lateral derecha sin incidencias' (term: episiotomia)\n",
            "'enfermedad renal cronica no filiada , de' (term: renal)\n",
            "'enfermedad renal cronica no filiada , de larga' (term: cronica)\n",
            "'sin signos de focalidad neurologica aguda .' (term: aguda)\n",
            "\n",
            "=== EXAMPLES OF UNCERTAIN MEDICAL TERMS ===\n",
            "'elevacion de reactantes de fase aguda compatible con cuadro infeccioso gastrointestinal' (term: aguda)\n",
            "'sospecha de apendicitis aguda .' (term: aguda)\n",
            "'bajo la sospecha de posible polineuropatia multifactorial toxicometabolica y carencial yo' (term: polineuropatia)\n",
            "'la sospecha de posible polineuropatia multifactorial toxicometabolica y carencial yo mielopatia' (term: multifactorial)\n",
            "'completar estudio de probable hepatopatia cronica por alcohol se realiza ecografia' (term: cronica)\n",
            "'orientacio diagnostica polineuropatia toxicometabolica y carencial posible milopatia' (term: polineuropatia)\n",
            "'placenta y membranas aparentemente integras .' (term: membranas)\n",
            "'la acm izquierda que sugiere infarto de cronologia aguda .' (term: infarto)\n",
            "'que sugiere infarto de cronologia aguda .' (term: aguda)\n",
            "', mrs5 orientacio diagnostica infarto isquemico taci acm i de probable' (term: isquemico)\n",
            "'probable etiologia cardioembolica cardiopatia isquemica cronica procediments tc craneal multimodal fibrinolisis' (term: cronica)\n",
            "'pared toracica anterior izq 11 insuficiencia renal que parece moderada proces' (term: insuficiencia)\n",
            "'toracica anterior izq 11 insuficiencia renal que parece moderada proces actual' (term: renal)\n",
            "'7.227.27 eb 0.92.9 placenta y membranas aparentemente integras .' (term: membranas)\n",
            "'proteinuria nefrotica y doble sistema renal izquierdo sugestivo de dudosa duplicacion' (term: renal)\n",
            "'antecedentes patologicos   hipertension arterial probablemente esencial en tratamiento' (term: hipertension)\n",
            "'antecedentes patologicos   hipertension arterial probablemente esencial en tratamiento con' (term: arterial)\n",
            "'perfil hepatico compatible con hepatopatia cronica con discreto empeoramiento respecto previa' (term: cronica)\n",
            "'enfermedad renal cronica de probable origen multifactorial' (term: renal)\n",
            "'enfermedad renal cronica de probable origen multifactorial con' (term: cronica)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Main program\n",
        "def main():\n",
        "    print(\"Loading SpaCy model...\")\n",
        "    nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "    print(\"Please upload your JSON file...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # Get the name of the uploaded file\n",
        "    filename = next(iter(uploaded))\n",
        "\n",
        "    # Open and load the JSON file\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Initialize counters for overall statistics\n",
        "    total_negation_counter = Counter()\n",
        "    total_uncertainty_counter = Counter()\n",
        "    total_medical_counter = Counter()\n",
        "\n",
        "    # Lists to store examples of negated and uncertain terms\n",
        "    negated_examples = []\n",
        "    uncertain_examples = []\n",
        "\n",
        "    # Process each record in the dataset\n",
        "    print(f\"Processing {len(data)} records...\")\n",
        "    for i, record in enumerate(data):\n",
        "        if i % 10 == 0:  # Status update every 10 records\n",
        "            print(f\"Processing record {i+1}/{len(data)}...\")\n",
        "\n",
        "        text = record.get(\"data\", {}).get(\"text\", \"\")\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        # Analyze the text\n",
        "        results = analyze_medical_context(text, nlp)\n",
        "\n",
        "        # Update overall counters\n",
        "        total_negation_counter.update(results[\"negation_cues_used\"])\n",
        "        total_uncertainty_counter.update(results[\"uncertainty_cues_used\"])\n",
        "        total_medical_counter.update(results[\"medical_terms_found\"])\n",
        "\n",
        "        # Store examples of negated and uncertain terms (up to 5 of each)\n",
        "        for neg_term in results[\"negated_terms\"]:\n",
        "            if len(negated_examples) < 20:  # Collect up to 20 examples\n",
        "                negated_examples.append(f\"'{neg_term['context']}' (term: {neg_term['term']})\")\n",
        "\n",
        "        for unc_term in results[\"uncertain_terms\"]:\n",
        "            if len(uncertain_examples) < 20:  # Collect up to 20 examples\n",
        "                uncertain_examples.append(f\"'{unc_term['context']}' (term: {unc_term['term']})\")\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n=== STATISTICS ===\")\n",
        "\n",
        "    print(\"\\nNegation Cue Frequencies (affecting medical terms):\")\n",
        "    for word, freq in total_negation_counter.most_common():\n",
        "        print(f\"{word}: {freq}\")\n",
        "\n",
        "    print(\"\\nUncertainty Cue Frequencies (affecting medical terms):\")\n",
        "    for word, freq in total_uncertainty_counter.most_common():\n",
        "        print(f\"{word}: {freq}\")\n",
        "\n",
        "    print(\"\\nMedical Terms Frequencies:\")\n",
        "    for word, freq in total_medical_counter.most_common():\n",
        "        print(f\"{word}: {freq}\")\n",
        "\n",
        "    # Print examples of negated and uncertain terms\n",
        "    print(\"\\n=== EXAMPLES OF NEGATED MEDICAL TERMS ===\")\n",
        "    for example in negated_examples:\n",
        "        print(example)\n",
        "\n",
        "    print(\"\\n=== EXAMPLES OF UNCERTAIN MEDICAL TERMS ===\")\n",
        "    for example in uncertain_examples:\n",
        "        print(example)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code 2"
      ],
      "metadata": {
        "id": "faOacJcpTBXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing Required Libraries:**\n"
      ],
      "metadata": {
        "id": "31MQQYQrs7mV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install spacy unidecode\n",
        "!python -m spacy download es_core_news_sm\n",
        "!python -m spacy download ca_core_news_sm\n"
      ],
      "metadata": {
        "id": "PdpI1R56AyrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e80e16-d148-4db4-d0fb-a1393077427d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting ca-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ca_core_news_sm-3.8.0/ca_core_news_sm-3.8.0-py3-none-any.whl (19.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ca-core-news-sm\n",
            "Successfully installed ca-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ca_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing libraries for text processing, NLP, and file handling in Colab**"
      ],
      "metadata": {
        "id": "8A1TZILMl12T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "import spacy\n",
        "from google.colab import files\n",
        "from unidecode import unidecode\n",
        "from spacy.lang.es.stop_words import STOP_WORDS as stopwords_es\n",
        "from spacy.lang.ca.stop_words import STOP_WORDS as stopwords_ca"
      ],
      "metadata": {
        "id": "iST8TFTk0Q18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the pre-trained spaCy models for Spanish and Catalan to handle language-specific processing:\n"
      ],
      "metadata": {
        "id": "pj8WMKRBtecR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load language models\n",
        "nlp_es = spacy.load(\"es_core_news_sm\")\n",
        "nlp_ca = spacy.load(\"ca_core_news_sm\")"
      ],
      "metadata": {
        "id": "4UAMwN9K1GvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition of our vocabulary lists:**\n",
        " negation cues, uncertainty expressions, and medical terms in Spanish and Catalan. These will be used later to identify and classify relevant patterns in the text.\n"
      ],
      "metadata": {
        "id": "gWBtWcQHtrAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define negation, uncertainty and UMLS medical terms\n",
        "NEGATION_WORDS = [\n",
        "      # Common negation words\n",
        "      \"no\", \"sin\", \"ausencia de\", \"descarta\", \"descartado\", \"excluye\", \"excluido\", \"niega\", \"negado\",\n",
        "      \"negativa\", \"negación\", \"ningún\", \"ninguna\", \"ninguno\", \"imposible\", \"inhallable\", \"carece de\", \"nunca\",\n",
        "      \"jamás\", \"tampoco\", \"ni\", \"nada\", \"negativo\", \"mai\",\n",
        "\n",
        "      # Medical-specific negation in Spanish\n",
        "      \"sin evidencia de\", \"no se observa\", \"no presenta\", \"no muestra\", \"no evidencia\", \"no compatible con\",\n",
        "      \"no concluyente\", \"no parece\", \"no se detecta\", \"sin signos de\", \"sin síntomas de\", \"sin indicios de\",\n",
        "      \"sin hallazgos de\", \"sin pruebas de\", \"sin rastro de\", \"ausente\", \"no encontrado\", \"sin cambios\",\n",
        "      \"no se aprecian\", \"no se ven\", \"descartando\", \"descartable\", \"no hay evidencia de\", \"no hay indicación de\",\n",
        "      \"libre de\", \"exento de\", \"sin manifestaciones de\", \"se excluye\", \"queda descartado\", \"ninguna evidencia de\",\n",
        "      \"ningún signo de\", \"sin afección\", \"no identificado\", \"negado por el paciente\", \"negado clínicamente\",\n",
        "      \"sin enfermedad\", \"sin afectación\", \"no afectado\", \"no positivo\", \"resultado negativo\",\n",
        "      \"resultado no reactivo\", \"resultado no positivo\",\n",
        "\n",
        "      # Medical-specific negation in Catalan\n",
        "      \"sense\", \"no es detecta\", \"no es veu\", \"no hi ha\", \"no presenta\", \"sense indicis de\", \"sense evidència de\",\n",
        "      \"sense senyals de\", \"sense rastre de\", \"sense afectació\", \"sense afecció\", \"no concloent\", \"sense canvis\",\n",
        "      \"sense resultats\", \"sense manifestacions de\", \"no s'observa\", \"no s'aprecia\", \"sense presència de\",\n",
        "      \"no compatible amb\", \"no és visible\", \"sense símptomes\", \"no diagnosticat\", \"sense senyals clars\",\n",
        "      \"diagnòstic negatiu\"\n",
        "  ]\n",
        "\n",
        "UNCERTAINTY_WORDS = [\n",
        "      # Common & medical uncertainty words in Spanish\n",
        "      \"posible\", \"quizás\", \"podría\", \"sospecha de\", \"considera\", \"probable\", \"aparentemente\", \"puede\", \"posiblemente\",\n",
        "      \"parece\", \"se considera\", \"indeterminado\", \"probabilidad de\", \"no concluyente\", \"eventual\", \"en estudio\",\n",
        "      \"pendiente de evaluación\", \"sugestivo de\", \"sugiere\", \"indica que\", \"se sospecha de\", \"podría indicar\",\n",
        "      \"dudoso\", \"no definido\", \"no específico\", \"no determinado\", \"valor incierto\", \"no claro\", \"no seguro\",\n",
        "      \"compatible con\", \"aparenta ser\", \"tendría que evaluarse\", \"a determinar\", \"probabilidad baja de\",\n",
        "      \"probabilidad alta de\", \"sin certeza\", \"hipotético\", \"hipotéticamente\", \"a confirmar\", \"falta de certeza\",\n",
        "      \"en posible relación con\", \"estaría asociado\", \"aparentemente relacionado con\", \"se intuye\", \"se deduce que\",\n",
        "      \"en consideración\", \"posible\", \"probablemente\", \"tal vez\", \"aproximadamente\", \"probable\",\n",
        "\n",
        "      # Common & medical uncertainty in Catalan\n",
        "      \"possible\", \"potser\", \"podria\", \"sospita de\", \"es considera\", \"probable\", \"aparentment\", \"pot ser\",\n",
        "      \"possiblement\", \"sembla\", \"es sospita de\", \"és indeterminat\", \"probabilitat de\", \"no concloent\", \"eventual\",\n",
        "      \"en estudi\", \"pendent d'avaluació\", \"suggerent de\", \"suggerix\", \"indica que\", \"dubtós\", \"no definit\",\n",
        "      \"no específic\", \"no determinat\", \"valor incert\", \"no clar\", \"no segur\", \"aparentment relacionat amb\",\n",
        "      \"es dedueix que\", \"en consideració\"\n",
        "  ]\n",
        "\n",
        "UMLS_MEDICAL_TERMS = [\n",
        "      \"uretrotomia\", \"interna\", \"cistoscopia\", \"estenosis\", \"uretra\", \"cronica\", \"diverticulosis\", \"insuficiencia\",\n",
        "      \"renal\", \"colelitiasis\", \"bloqueo\", \"auriculoventricular\", \"primer grado\", \"segundo grado\", \"hipertension\",\n",
        "      \"arterial\", \"protesis\", \"cadera\", \"cordectomia\", \"herniorrafia\", \"parto\", \"eutocico\", \"rotura\", \"membranas\",\n",
        "      \"prematuro\", \"episiotomia\", \"lactancia materna\", \"apendicectomia\", \"laparoscopica\", \"gastroenteritis\", \"aguda\",\n",
        "      \"nefrectomia\", \"parcial\", \"angiomiolipoma\", \"quistes\", \"renales\", \"fractura\", \"mandibular\", \"ictus\", \"infarto\",\n",
        "      \"isquemico\", \"trombectomia\", \"cerebral\", \"fibrinolisis\", \"endovenosa\", \"colangitis\", \"microcirugia\",\n",
        "      \"endolaringea\", \"polineuropatia\", \"sensitiva\", \"axonal\", \"neuropatia\", \"multifactorial\", \"mielopatia\", \"déficit\"\n",
        "  ]"
      ],
      "metadata": {
        "id": "ohQj-vRM41A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also want to implement a helper function to detect double negation within a context window, to avoid misclassifications when multiple cues appear together:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xFm32YoYt1TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect double negation in a window around a medical term\n",
        "def is_double_negation(tokens):\n",
        "    \"\"\"Check if a sequence of tokens contains double negation\"\"\"\n",
        "    # Simple cues that would indicate negation (a subset of the full NEGATION_WORDS)\n",
        "    simple_negation_cues = {\"no\", \"sin\", \"nunca\", \"jamás\", \"ningún\", \"ninguna\", \"nadie\", \"ninguno\", \"negado\", \"niega\"}\n",
        "    negation_count = sum(1 for token in tokens if token.lower() in simple_negation_cues)\n",
        "    return negation_count >= 2"
      ],
      "metadata": {
        "id": "ghvTG_pV5C12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create two functions: one to normalize the text (removing accents, symbols, and extra spaces), and another to preprocess it by tokenizing and filtering stopwords (while keeping negation cues!!)\n"
      ],
      "metadata": {
        "id": "ifYSUgD9uHdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    \"\"\"Normalize medical text with Spanish/Catalan character support\"\"\"\n",
        "    text = text.lower()\n",
        "    text = unidecode(text)  # Remove accents\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'\\*+', '', text)  # Remove sensitive patient identifiers\n",
        "    text = re.sub(r'  ', '', text)\n",
        "    #text = re.sub(r'[^\\w\\s.,;:!?-]', '', text)  # Remove unnecessary punctuation\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text, lang=\"es\"):\n",
        "    \"\"\"\n",
        "    Preprocess text by tokenizing, removing stopwords (except negation words), and normalizing case.\n",
        "    Returns:\n",
        "        List of clean tokens with negation words preserved.\n",
        "    \"\"\"\n",
        "    text = normalize_text(text)\n",
        "\n",
        "    if lang == \"es\":\n",
        "        nlp = nlp_es\n",
        "        stopwords = stopwords_es\n",
        "    else:\n",
        "        nlp = nlp_ca\n",
        "        stopwords = stopwords_ca\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    tokens = [\n",
        "        token.text for token in doc\n",
        "        if token.is_alpha and (token.text not in stopwords or token.text in NEGATION_WORDS)\n",
        "    ]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "71DIj-k-1jyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great!\n",
        "\n",
        "Now we can implement the function that analyzes a medical text: it tokenizes the input, looks for known medical terms, and checks whether they appear in a context of negation, uncertainty, or double negation\n"
      ],
      "metadata": {
        "id": "fsroB0cYuhjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_medical_context(text, lang=\"es\"):\n",
        "    \"\"\"Analyze medical text for negation, uncertainty, and medical terms.\"\"\"\n",
        "    tokens = preprocess_text(text, lang)\n",
        "    processed_text = \" \".join(tokens)\n",
        "    doc = nlp_es(processed_text) if lang == \"es\" else nlp_ca(processed_text)\n",
        "\n",
        "    results = {\n",
        "        \"negated_terms\": [],\n",
        "        \"uncertain_terms\": [],\n",
        "        \"double_negated_terms\": [],\n",
        "        \"negation_cues_used\": Counter(),\n",
        "        \"uncertainty_cues_used\": Counter(),\n",
        "        \"medical_terms_found\": Counter()\n",
        "    }\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        sent_tokens = [token.text.lower() for token in sent]\n",
        "\n",
        "        for i, token_text in enumerate(sent_tokens):\n",
        "            if token_text in [term.lower() for term in UMLS_MEDICAL_TERMS]:\n",
        "                results[\"medical_terms_found\"][token_text] += 1\n",
        "\n",
        "                start_idx = max(0, i - 5)\n",
        "                end_idx = min(len(sent_tokens), i + 6)\n",
        "                context_tokens = sent_tokens[start_idx:end_idx]\n",
        "                context_text = \" \".join(context_tokens)\n",
        "\n",
        "                if is_double_negation(context_tokens):\n",
        "                    results[\"double_negated_terms\"].append({\"term\": token_text, \"context\": context_text})\n",
        "                    continue\n",
        "\n",
        "                negated = any(neg in context_text for neg in NEGATION_WORDS)\n",
        "                if negated:\n",
        "                    results[\"negation_cues_used\"].update([neg for neg in NEGATION_WORDS if neg in context_text])\n",
        "                    results[\"negated_terms\"].append({\"term\": token_text, \"context\": context_text})\n",
        "                    continue\n",
        "\n",
        "                uncertain = any(unc in context_text for unc in UNCERTAINTY_WORDS)\n",
        "                if uncertain:\n",
        "                    results[\"uncertainty_cues_used\"].update([unc for unc in UNCERTAINTY_WORDS if unc in context_text])\n",
        "                    results[\"uncertain_terms\"].append({\"term\": token_text, \"context\": context_text})\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "GZRkVHOtAt_8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running the Rule-based approach on the train data:**"
      ],
      "metadata": {
        "id": "jMTB_dAz-Mhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final step is to run our full rule based system on the dataset.\n",
        "\n",
        "Let’s see how it goes!"
      ],
      "metadata": {
        "id": "JmZStcTn3kP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Please upload your JSON file...\")\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "total_negation_counter = Counter()\n",
        "total_uncertainty_counter = Counter()\n",
        "total_medical_counter = Counter()\n",
        "\n",
        "negated_examples = []\n",
        "uncertain_examples = []\n",
        "\n",
        "print(f\"Processing {len(data)} records...\")\n",
        "for i, record in enumerate(data):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Processing record {i+1}/{len(data)}...\")\n",
        "\n",
        "    text = record.get(\"data\", {}).get(\"text\", \"\")\n",
        "    if not text:\n",
        "        continue\n",
        "\n",
        "    results = analyze_medical_context(text)\n",
        "\n",
        "    total_negation_counter.update(results[\"negation_cues_used\"])\n",
        "    total_uncertainty_counter.update(results[\"uncertainty_cues_used\"])\n",
        "    total_medical_counter.update(results[\"medical_terms_found\"])\n",
        "\n",
        "    negated_examples.extend([f\"'{term['context']}' (term: {term['term']})\" for term in results[\"negated_terms\"][:20]])\n",
        "    uncertain_examples.extend([f\"'{term['context']}' (term: {term['term']})\" for term in results[\"uncertain_terms\"][:20]])\n",
        "\n",
        "print(\"\\n=== STATISTICS ===\")\n",
        "\n",
        "print(\"\\nNegation Cue Frequencies:\")\n",
        "for word, freq in total_negation_counter.most_common():\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "print(\"\\nUncertainty Cue Frequencies:\")\n",
        "for word, freq in total_uncertainty_counter.most_common():\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "print(\"\\nMedical Terms Frequencies:\")\n",
        "for word, freq in total_medical_counter.most_common():\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "print(\"\\n=== EXAMPLES OF NEGATED MEDICAL TERMS ===\")\n",
        "for example in negated_examples[:20]:\n",
        "    print(example)\n",
        "\n",
        "print(\"\\n=== EXAMPLES OF UNCERTAIN MEDICAL TERMS ===\")\n",
        "for example in uncertain_examples[:20]:\n",
        "    print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EMSr3P461zml",
        "outputId": "e23d71af-9201-40b9-8a3d-4c1b1dce993e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your JSON file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d3efeccb-4be2-45df-aab6-317fac3996b8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d3efeccb-4be2-45df-aab6-317fac3996b8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving negacio_train_v2024.json to negacio_train_v2024 (1).json\n",
            "Processing 254 records...\n",
            "Processing record 1/254...\n",
            "Processing record 11/254...\n",
            "Processing record 21/254...\n",
            "Processing record 31/254...\n",
            "Processing record 41/254...\n",
            "Processing record 51/254...\n",
            "Processing record 61/254...\n",
            "Processing record 71/254...\n",
            "Processing record 81/254...\n",
            "Processing record 91/254...\n",
            "Processing record 101/254...\n",
            "Processing record 111/254...\n",
            "Processing record 121/254...\n",
            "Processing record 131/254...\n",
            "Processing record 141/254...\n",
            "Processing record 151/254...\n",
            "Processing record 161/254...\n",
            "Processing record 171/254...\n",
            "Processing record 181/254...\n",
            "Processing record 191/254...\n",
            "Processing record 201/254...\n",
            "Processing record 211/254...\n",
            "Processing record 221/254...\n",
            "Processing record 231/254...\n",
            "Processing record 241/254...\n",
            "Processing record 251/254...\n",
            "\n",
            "=== STATISTICS ===\n",
            "\n",
            "Negation Cue Frequencies:\n",
            "no: 794\n",
            "ni: 707\n",
            "sin: 228\n",
            "negativo: 14\n",
            "sense: 14\n",
            "negativa: 12\n",
            "descarta: 12\n",
            "niega: 10\n",
            "nada: 10\n",
            "no presenta: 8\n",
            "no muestra: 4\n",
            "ausencia de: 2\n",
            "sin cambios: 2\n",
            "no evidencia: 1\n",
            "descartando: 1\n",
            "\n",
            "Uncertainty Cue Frequencies:\n",
            "probable: 18\n",
            "aparentemente: 13\n",
            "probablemente: 2\n",
            "possible: 2\n",
            "possiblement: 1\n",
            "\n",
            "Medical Terms Frequencies:\n",
            "parto: 250\n",
            "renal: 198\n",
            "aguda: 154\n",
            "cronica: 97\n",
            "arterial: 85\n",
            "fractura: 80\n",
            "hipertension: 67\n",
            "interna: 63\n",
            "insuficiencia: 61\n",
            "parcial: 46\n",
            "estenosis: 42\n",
            "ictus: 42\n",
            "protesis: 41\n",
            "cerebral: 41\n",
            "eutocico: 40\n",
            "endovenosa: 36\n",
            "membranas: 33\n",
            "infarto: 33\n",
            "apendicectomia: 27\n",
            "nefrectomia: 27\n",
            "laparoscopica: 26\n",
            "cadera: 21\n",
            "episiotomia: 20\n",
            "rotura: 19\n",
            "renales: 18\n",
            "cistoscopia: 16\n",
            "isquemico: 16\n",
            "uretra: 14\n",
            "bloqueo: 13\n",
            "quistes: 12\n",
            "prematuro: 12\n",
            "herniorrafia: 11\n",
            "mandibular: 10\n",
            "sensitiva: 9\n",
            "colangitis: 8\n",
            "neuropatia: 7\n",
            "axonal: 7\n",
            "trombectomia: 7\n",
            "diverticulosis: 6\n",
            "colelitiasis: 6\n",
            "multifactorial: 6\n",
            "gastroenteritis: 5\n",
            "cordectomia: 4\n",
            "polineuropatia: 4\n",
            "uretrotomia: 3\n",
            "angiomiolipoma: 3\n",
            "microcirugia: 3\n",
            "endolaringea: 3\n",
            "fibrinolisis: 3\n",
            "mielopatia: 2\n",
            "auriculoventricular: 1\n",
            "\n",
            "=== EXAMPLES OF NEGATED MEDICAL TERMS ===\n",
            "'paciente ingresa forma programada realizacion uretrotomia interna antecedents alergia penicilina cloramfenicol' (term: uretrotomia)\n",
            "'ingresa forma programada realizacion uretrotomia interna antecedents alergia penicilina cloramfenicol no' (term: interna)\n",
            "'no habitos toxicos antecedentes medicos bloqueo auriculoventricular grado hipertension arterial diverticulosis' (term: bloqueo)\n",
            "'medicos bloqueo auriculoventricular grado hipertension arterial diverticulosis extensa insuficiencia renal cronica' (term: arterial)\n",
            "'bloqueo auriculoventricular grado hipertension arterial diverticulosis extensa insuficiencia renal cronica colelitiasis' (term: diverticulosis)\n",
            "'grado hipertension arterial diverticulosis extensa insuficiencia renal cronica colelitiasis antecedentes quirurgicos' (term: insuficiencia)\n",
            "'hipertension arterial diverticulosis extensa insuficiencia renal cronica colelitiasis antecedentes quirurgicos exeresis' (term: renal)\n",
            "'arterial diverticulosis extensa insuficiencia renal cronica colelitiasis antecedentes quirurgicos exeresis lesiones' (term: cronica)\n",
            "'diverticulosis extensa insuficiencia renal cronica colelitiasis antecedentes quirurgicos exeresis lesiones cutaneas' (term: colelitiasis)\n",
            "'exeresis lesiones cutaneas anestesia local protesis cadera cordectomia herniorrafia inguinal proces' (term: protesis)\n",
            "'lesiones cutaneas anestesia local protesis cadera cordectomia herniorrafia inguinal proces actual' (term: cadera)\n",
            "'cutaneas anestesia local protesis cadera cordectomia herniorrafia inguinal proces actual varon' (term: cordectomia)\n",
            "'anestesia local protesis cadera cordectomia herniorrafia inguinal proces actual varon raiz' (term: herniorrafia)\n",
            "'raiz episodio hematuria macroscopica realiza cistoscopia negativa lesiones malignas objetiva estenosis' (term: cistoscopia)\n",
            "'cistoscopia negativa lesiones malignas objetiva estenosis uretra intentan dilataciones progresivas gabinete' (term: estenosis)\n",
            "'negativa lesiones malignas objetiva estenosis uretra intentan dilataciones progresivas gabinete urologia' (term: uretra)\n",
            "'solicita estudio imagen confirma existencia estenosis nivel d uretra bulbar indica' (term: estenosis)\n",
            "'confirma existencia estenosis nivel d uretra bulbar indica uretrtomia interna exploracio' (term: uretra)\n",
            "'retrograda cums uretrografia retrograda muestra uretra estenosis focales nivel uretra peneana' (term: uretra)\n",
            "'cums uretrografia retrograda muestra uretra estenosis focales nivel uretra peneana bulbar' (term: estenosis)\n",
            "\n",
            "=== EXAMPLES OF UNCERTAIN MEDICAL TERMS ===\n",
            "'ocular eritromicina alumbramiento dirigido placenta membranas aparentemente integras revisa canal blando' (term: membranas)\n",
            "'aparentemente integras revisa canal blando parto objetivandose desgarro grado i horquilla' (term: parto)\n",
            "'apgar peso ph eb placenta membranas aparentemente integras realiza sutura histerorrafia' (term: membranas)\n",
            "'rural industria textil antecedentes patologicos hipertension arterial probablemente esencial tratamiento iecas' (term: hipertension)\n",
            "'industria textil antecedentes patologicos hipertension arterial probablemente esencial tratamiento iecas tiazidas' (term: arterial)\n",
            "'cpre papilotomia limpieza complicaciones presento colangitis cultivos positivos bgp probable bacilus' (term: colangitis)\n",
            "'pus versus component hematic parenquima renal bilateral parenquima dreta possiblement per' (term: renal)\n",
            "'hipodensidad hemicerebelosa derecha probable relacion infarto establecido territorio pica derecha hipodensidades' (term: infarto)\n",
            "'id rn alumbramiento espontaneo placenta membranas aparentemente integras revisa canal blando' (term: membranas)\n",
            "'aparentemente integras revisa canal blando parto objetivandose episiotomia emld desgarro grado' (term: parto)\n",
            "'ocular eritromicina alumbramiento dirigido placenta membranas aparentemente integras revisa canal blando' (term: membranas)\n",
            "'aparentemente integras revisa canal blando parto objetivandose desgarro grado ii sutura' (term: parto)\n",
            "'fisioterapia i logopedia atribuit possible ictus viu barcelona des fa any' (term: ictus)\n",
            "'alumbramiento espontaneo minutos nacimiento placenta membranas aparentemente integras revision canal parto' (term: membranas)\n",
            "'ph peso alumbramiento dirigido placenta membranas aparentemente integras revisa canal blando' (term: membranas)\n",
            "'aparentemente integras revisa canal blando parto objetivandose episiotomia mediolateral derecha sutura' (term: parto)\n",
            "'aguda rash cutaneo urticariforme orienta gastroenteritis aguda probable etiologia viral pautandose' (term: gastroenteritis)\n",
            "'rash cutaneo urticariforme orienta gastroenteritis aguda probable etiologia viral pautandose sueroterapia' (term: aguda)\n",
            "'aparentemente integra revisa canal blando parto episiotomia mediolateral derecha desgarro musculo' (term: parto)\n",
            "'ocular eritromicina alumbramiento dirigido placenta membranas aparentemente integras revisa canal blando' (term: membranas)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## HYBRID VERSION OF THE TWO PREVIOUS CODES\n",
        "\n",
        "**We will now run the combined version of both codes.**\n",
        "\n",
        "We use the **`normalize_text`** function from Code 2, and bring in the pattern-based rules from Code 1 including helper functions like **`detect_multiword_pattern`** and the four scope detection strategies: prefix, postfix, UMLS term + negation phrase, and negation phrase + UMLS term.\n",
        "\n",
        "The output will include updated counters, matched examples, and final stats to evaluate how this hybrid version performs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tTbrbP3shzu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(We already have the packages installed from the previous codes)"
      ],
      "metadata": {
        "id": "3j_TX-FTxJLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing libraries for text processing, NLP, and file handling in Colab**"
      ],
      "metadata": {
        "id": "CcRLXDGuxJkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter, defaultdict\n",
        "import spacy\n",
        "from google.colab import files\n",
        "import time"
      ],
      "metadata": {
        "id": "ATXzxPDr5hET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s use our key word lists again: negation cues, uncertainty expressions, and medical terms. This will be the base of our combined system for tagging relevant patterns in the clinical texts.\n"
      ],
      "metadata": {
        "id": "-6W2csjdxvsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define negation, uncertainty and UMLS medical terms\n",
        "NEGATION_WORDS = [\n",
        "    # Common negation words\n",
        "    \"no\", \"sin\", \"ausencia de\", \"descarta\", \"descartado\", \"excluye\", \"excluido\", \"niega\", \"negado\",\n",
        "    \"negativa\", \"negación\", \"ningún\", \"ninguna\", \"ninguno\", \"imposible\", \"inhallable\", \"carece de\", \"nunca\",\n",
        "    \"jamás\", \"tampoco\", \"ni\", \"nada\", \"negativo\", \"mai\",\n",
        "\n",
        "    # Medical-specific negation in Spanish\n",
        "    \"sin evidencia de\", \"no se observa\", \"no presenta\", \"no muestra\", \"no evidencia\", \"no compatible con\",\n",
        "    \"no concluyente\", \"no parece\", \"no se detecta\", \"sin signos de\", \"sin síntomas de\", \"sin indicios de\",\n",
        "    \"sin hallazgos de\", \"sin pruebas de\", \"sin rastro de\", \"ausente\", \"no encontrado\", \"sin cambios\",\n",
        "    \"no se aprecian\", \"no se ven\", \"descartando\", \"descartable\", \"no hay evidencia de\", \"no hay indicación de\",\n",
        "    \"libre de\", \"exento de\", \"sin manifestaciones de\", \"se excluye\", \"queda descartado\", \"ninguna evidencia de\",\n",
        "    \"ningún signo de\", \"sin afección\", \"no identificado\", \"negado por el paciente\", \"negado clínicamente\",\n",
        "    \"sin enfermedad\", \"sin afectación\", \"no afectado\", \"no positivo\", \"resultado negativo\",\n",
        "    \"resultado no reactivo\", \"resultado no positivo\",\n",
        "\n",
        "    # Medical-specific negation in Catalan\n",
        "    \"sense\", \"no es detecta\", \"no es veu\", \"no hi ha\", \"no presenta\", \"sense indicis de\", \"sense evidència de\",\n",
        "    \"sense senyals de\", \"sense rastre de\", \"sense afectació\", \"sense afecció\", \"no concloent\", \"sense canvis\",\n",
        "    \"sense resultats\", \"sense manifestacions de\", \"no s'observa\", \"no s'aprecia\", \"sense presència de\",\n",
        "    \"no compatible amb\", \"no és visible\", \"sense símptomes\", \"no diagnosticat\", \"sense senyals clars\",\n",
        "    \"diagnòstic negatiu\"\n",
        "]\n",
        "\n",
        "UNCERTAINTY_WORDS = [\n",
        "    # Common & medical uncertainty words in Spanish\n",
        "    \"posible\", \"quizás\", \"podría\", \"sospecha de\", \"considera\", \"probable\", \"aparentemente\", \"puede\", \"posiblemente\",\n",
        "    \"parece\", \"se considera\", \"indeterminado\", \"probabilidad de\", \"no concluyente\", \"eventual\", \"en estudio\",\n",
        "    \"pendiente de evaluación\", \"sugestivo de\", \"sugiere\", \"indica que\", \"se sospecha de\", \"podría indicar\",\n",
        "    \"dudoso\", \"no definido\", \"no específico\", \"no determinado\", \"valor incierto\", \"no claro\", \"no seguro\",\n",
        "    \"compatible con\", \"aparenta ser\", \"tendría que evaluarse\", \"a determinar\", \"probabilidad baja de\",\n",
        "    \"probabilidad alta de\", \"sin certeza\", \"hipotético\", \"hipotéticamente\", \"a confirmar\", \"falta de certeza\",\n",
        "    \"en posible relación con\", \"estaría asociado\", \"aparentemente relacionado con\", \"se intuye\", \"se deduce que\",\n",
        "    \"en consideración\", \"posible\", \"probablemente\", \"tal vez\", \"aproximadamente\", \"probable\",\n",
        "\n",
        "    # Common & medical uncertainty in Catalan\n",
        "    \"possible\", \"potser\", \"podria\", \"sospita de\", \"es considera\", \"probable\", \"aparentment\", \"pot ser\",\n",
        "    \"possiblement\", \"sembla\", \"es sospita de\", \"és indeterminat\", \"probabilitat de\", \"no concloent\", \"eventual\",\n",
        "    \"en estudi\", \"pendent d'avaluació\", \"suggerent de\", \"suggerix\", \"indica que\", \"dubtós\", \"no definit\",\n",
        "    \"no específic\", \"no determinat\", \"valor incert\", \"no clar\", \"no segur\", \"aparentment relacionat amb\",\n",
        "    \"es dedueix que\", \"en consideració\"\n",
        "]\n",
        "\n",
        "UMLS_MEDICAL_TERMS = [\n",
        "    \"uretrotomia\", \"interna\", \"cistoscopia\", \"estenosis\", \"uretra\", \"cronica\", \"diverticulosis\", \"insuficiencia\",\n",
        "    \"renal\", \"colelitiasis\", \"bloqueo\", \"auriculoventricular\", \"primer grado\", \"segundo grado\", \"hipertension\",\n",
        "    \"arterial\", \"protesis\", \"cadera\", \"cordectomia\", \"herniorrafia\", \"parto\", \"eutocico\", \"rotura\", \"membranas\",\n",
        "    \"prematuro\", \"episiotomia\", \"lactancia materna\", \"apendicectomia\", \"laparoscopica\", \"gastroenteritis\", \"aguda\",\n",
        "    \"nefrectomia\", \"parcial\", \"angiomiolipoma\", \"quistes\", \"renales\", \"fractura\", \"mandibular\", \"ictus\", \"infarto\",\n",
        "    \"isquemico\", \"trombectomia\", \"cerebral\", \"fibrinolisis\", \"endovenosa\", \"colangitis\", \"microcirugia\",\n",
        "    \"endolaringea\", \"polineuropatia\", \"sensitiva\", \"axonal\", \"neuropatia\", \"multifactorial\", \"mielopatia\", \"déficit\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "Jw0RsQzn5kyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text normalization**\n",
        "\n",
        "Next, we clean up the input text by normalizing it removing special characters, extra spaces, and anything that might interfere with proper analysis.\n"
      ],
      "metadata": {
        "id": "Hamjgs4-yDQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-processing\n",
        "def normalize_text(text):\n",
        "    \"\"\"Normalize medical text with Spanish/Catalan character support\"\"\"\n",
        "    text = text.lower()\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'\\*+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?-àáèéìíòóùúüñç]', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "oORHauXd6VhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper functions for detection and ground truth**  \n",
        "\n",
        "Here we will define three utility functions to support our analysis: detecting double negation, handling suffix-based medical terms, and extracting annotated ground truth for evaluation:\n"
      ],
      "metadata": {
        "id": "cdzsXo3Dyizi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def is_double_negation(tokens):\n",
        "    \"\"\" Check if a sequence of tokens contains at least two simple negation cues \"\"\"\n",
        "    simple_negation_cues = {\"no\", \"sin\", \"nunca\", \"jamás\", \"ningún\", \"ninguna\", \"nadie\", \"ninguno\", \"negado\", \"niega\"}\n",
        "    negation_count = sum(1 for token in tokens if token in simple_negation_cues)\n",
        "    return negation_count >= 2\n",
        "\n",
        "def get_extended_umls_terms(umls_terms, suffix_counter):\n",
        "    \"\"\" Combina la lista original de UMLS con los términos detectados vía sufijo sin repetir (todo en minúsculas)\"\"\"\n",
        "    extended = set(term.lower() for term in umls_terms)\n",
        "    for term in suffix_counter:\n",
        "        if term.lower() not in extended:\n",
        "            extended.add(term.lower())\n",
        "    return extended\n",
        "\n",
        "def detect_medical_suffix(token):\n",
        "    \"\"\"Detect if a token ends with a sufijo médico específico: -nosis, -tomia, -patia, -losis\"\"\"\n",
        "    pattern = re.compile(r'.*(nosis|tomia|patia|losis)$')\n",
        "    return pattern.match(token)\n",
        "\n",
        "def extract_ground_truth(record):\n",
        "    \"\"\"Extract ground truth negation and uncertainty terms from annotations\"\"\"\n",
        "    gt_neg = set()\n",
        "    gt_unc = set()\n",
        "    annotations = record.get(\"annotations\", [])\n",
        "    for ann in annotations:\n",
        "        for res in ann.get(\"result\", []):\n",
        "            labels = res.get(\"value\", {}).get(\"labels\", [])\n",
        "            term = res.get(\"value\", {}).get(\"text\", \"\").lower()\n",
        "            if \"NEG\" in labels:\n",
        "                gt_neg.add(term)\n",
        "            if \"UNC\" in labels:\n",
        "                gt_unc.add(term)\n",
        "    return gt_neg, gt_unc\n"
      ],
      "metadata": {
        "id": "nSVKV5jd5rPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation metrics**  \n",
        "We will now impkement a function to calculate precision, recall, and F1 score.Which are basic but essential metrics to evaluate how well our system detects negation and uncertainty compared to the annotated ground truth\n"
      ],
      "metadata": {
        "id": "_6HL6ABrysbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics\n",
        "def compute_metrics(predicted, ground_truth):\n",
        "    \"\"\"Calcula precisión, recall y F1\"\"\"\n",
        "    tp = len(predicted & ground_truth)\n",
        "    fp = len(predicted - ground_truth)\n",
        "    fn = len(ground_truth - predicted)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1        = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    return precision, recall, f1"
      ],
      "metadata": {
        "id": "xRKA-14v6xt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main analysis function**  \n",
        "This next function will bring everything together: it processes each sentence, detects medical terms, checks for suffix patterns and analyzes their surrounding context to flag negation, uncertainty, or double negation.\n",
        "\n",
        "It also updates all relevant counters so we can evaluate later.\n"
      ],
      "metadata": {
        "id": "b-OR6LdQzCkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_medical_context(text, nlp):\n",
        "    normalized_text = normalize_text(text)\n",
        "    doc = nlp(normalized_text)\n",
        "    results = {\n",
        "        \"negated_terms\": [],\n",
        "        \"uncertain_terms\": [],\n",
        "        \"double_negated_terms\": [],\n",
        "        \"negation_cues_used\": Counter(),\n",
        "        \"uncertainty_cues_used\": Counter(),\n",
        "        \"medical_terms_found\": Counter(),\n",
        "        \"medical_suffix_terms\": Counter()\n",
        "    }\n",
        "\n",
        "    # Conjunto para rastrear contextos ya procesados\n",
        "    processed_contexts = set()\n",
        "\n",
        "    # Procesar cada oración\n",
        "    for sent in doc.sents:\n",
        "        sent_tokens = [token.text.lower() for token in sent]\n",
        "\n",
        "        # Detectar tokens que contengan sufijos médicos y acumularlos\n",
        "        for token in sent_tokens:\n",
        "            if detect_medical_suffix(token):\n",
        "                results[\"medical_suffix_terms\"][token] += 1\n",
        "\n",
        "        # Procesar coincidencias con términos UMLS\n",
        "        for i, token_text in enumerate(sent_tokens):\n",
        "            if token_text in [term.lower() for term in UMLS_MEDICAL_TERMS]:\n",
        "                results[\"medical_terms_found\"][token_text] += 1\n",
        "                start_idx = max(0, i - 5)\n",
        "                end_idx = min(len(sent_tokens), i + 6)\n",
        "                context_tokens = sent_tokens[start_idx:end_idx]\n",
        "                context_text = \" \".join(context_tokens)\n",
        "\n",
        "                # Verificar si este contexto ya ha sido procesado\n",
        "                if context_text in processed_contexts:\n",
        "                    continue\n",
        "\n",
        "                # Marcar este contexto como procesado\n",
        "                processed_contexts.add(context_text)\n",
        "\n",
        "                if is_double_negation(context_tokens):\n",
        "                    results[\"double_negated_terms\"].append({\"term\": token_text, \"context\": context_text})\n",
        "                    continue\n",
        "\n",
        "                if any(neg in context_text for neg in NEGATION_WORDS):\n",
        "                    results[\"negation_cues_used\"].update([neg for neg in NEGATION_WORDS if neg in context_text])\n",
        "                    results[\"negated_terms\"].append({\"term\": token_text, \"context\": context_text})\n",
        "                    continue\n",
        "\n",
        "                if any(unc in context_text for unc in UNCERTAINTY_WORDS):\n",
        "                    results[\"uncertainty_cues_used\"].update([unc for unc in UNCERTAINTY_WORDS if unc in context_text])\n",
        "                    results[\"uncertain_terms\"].append({\"term\": token_text, \"context\": context_text})\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "yud7pTMZ77LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final execution and evaluation**  \n",
        "Here we will run the complete combined system on the dataset. It processes each record, applies our detection logic, tracks statistics, and evaluates performance using precision, recall, and F1 score both for negation and uncertainty.\n",
        "And we also print sample outputs and processing time to assess efficiency.\n"
      ],
      "metadata": {
        "id": "FpjiQ10zzg0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading SpaCy model...\")\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "print(\"Please upload your JSON file...\")\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))\n",
        "with open(filename, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "total_negation_counter = Counter()\n",
        "total_uncertainty_counter = Counter()\n",
        "total_medical_counter = Counter()\n",
        "total_suffix_counter = Counter()\n",
        "\n",
        "# Conjuntos para realizar un seguimiento de los contextos únicos de los ejemplos\n",
        "negated_examples = []\n",
        "uncertain_examples = []\n",
        "used_contexts = set()\n",
        "\n",
        "# Variables para evaluación\n",
        "all_predicted_neg = set()\n",
        "all_ground_truth_neg = set()\n",
        "all_predicted_unc = set()\n",
        "all_ground_truth_unc = set()\n",
        "\n",
        "start_time = time.time()\n",
        "print(f\"Processing {len(data)} records...\")\n",
        "for i, record in enumerate(data):\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Processing record {i+1}/{len(data)}...\")\n",
        "    text = record.get(\"data\", {}).get(\"text\", \"\")\n",
        "    if not text:\n",
        "        continue\n",
        "\n",
        "    results = analyze_medical_context(text, nlp)\n",
        "    total_negation_counter.update(results[\"negation_cues_used\"])\n",
        "    total_uncertainty_counter.update(results[\"uncertainty_cues_used\"])\n",
        "    total_medical_counter.update(results[\"medical_terms_found\"])\n",
        "    total_suffix_counter.update(results[\"medical_suffix_terms\"])\n",
        "\n",
        "    # Procesamos los términos individuales evitando contextos duplicados\n",
        "    for neg_term in results[\"negated_terms\"]:\n",
        "        context = neg_term['context']\n",
        "        norm_term = normalize_text(neg_term[\"term\"])\n",
        "        if context not in used_contexts and len(negated_examples) < 20:\n",
        "            negated_examples.append(f\"'{context}' (term: {norm_term})\")\n",
        "            used_contexts.add(context)\n",
        "        all_predicted_neg.add(norm_term)\n",
        "\n",
        "    for unc_term in results[\"uncertain_terms\"]:\n",
        "        context = unc_term['context']\n",
        "        norm_term = normalize_text(unc_term[\"term\"])\n",
        "        if context not in used_contexts and len(uncertain_examples) < 20:\n",
        "            uncertain_examples.append(f\"'{context}' (term: {norm_term})\")\n",
        "            used_contexts.add(context)\n",
        "        all_predicted_unc.add(norm_term)\n",
        "\n",
        "    def extract_ground_truth(record):\n",
        "      ann = record.get(\"annotations\", {})\n",
        "      gt_neg = set(normalize_text(term) for term in ann.get(\"negation\", []))\n",
        "      gt_unc = set(normalize_text(term) for term in ann.get(\"uncertainty\", []))\n",
        "      return gt_neg, gt_unc\n",
        "\n",
        "end_time = time.time()\n",
        "processing_time = end_time - start_time\n",
        "\n",
        "prec_neg, rec_neg, f1_neg = compute_metrics(all_predicted_neg, all_ground_truth_neg)\n",
        "prec_unc, rec_unc, f1_unc = compute_metrics(all_predicted_unc, all_ground_truth_unc)\n",
        "\n",
        "combined_predicted = all_predicted_neg.union(all_predicted_unc)\n",
        "combined_ground_truth = all_ground_truth_neg.union(all_ground_truth_unc)\n",
        "prec_comb, rec_comb, f1_comb = compute_metrics(combined_predicted, combined_ground_truth)\n",
        "\n",
        "# Construir la lista extendida de términos UMLS (sin duplicados)\n",
        "extended_umls_terms = get_extended_umls_terms(UMLS_MEDICAL_TERMS, total_suffix_counter)\n",
        "\n",
        "# Crear un contador extendido que sume frecuencias de los términos de la lista original y los sufijos\n",
        "extended_terms_freq = defaultdict(int)\n",
        "for term, freq in total_medical_counter.items():\n",
        "    extended_terms_freq[term.lower()] += freq\n",
        "for term, freq in total_suffix_counter.items():\n",
        "    extended_terms_freq[term.lower()] += freq\n",
        "\n",
        "print(\"\\n=== STATISTICS ===\")\n",
        "print(\"\\nNegation Cue Frequencies (affecting medical terms):\")\n",
        "for word, freq in total_negation_counter.most_common():\n",
        "    print(f\"{word}: {freq}\")\n",
        "print(\"\\nUncertainty Cue Frequencies (affecting medical terms):\")\n",
        "for word, freq in total_uncertainty_counter.most_common():\n",
        "    print(f\"{word}: {freq}\")\n",
        "print(\"\\nMedical Terms Frequencies:\")\n",
        "for word, freq in total_medical_counter.most_common():\n",
        "    print(f\"{word}: {freq}\")\n",
        "print(\"\\nMedical Suffix Terms Frequencies:\")\n",
        "for word, freq in total_suffix_counter.most_common():\n",
        "    print(f\"{word}: {freq}\")\n",
        "\n",
        "print(\"\\n=== EXTENDED UMLS TERMS ===\")\n",
        "for term in sorted(extended_umls_terms):\n",
        "    print(term, f\"(total frequency: {extended_terms_freq[term]})\")\n",
        "\n",
        "print(\"\\n=== EXAMPLES OF NEGATED MEDICAL TERMS ===\")\n",
        "for example in negated_examples:\n",
        "    print(example)\n",
        "print(\"\\n=== EXAMPLES OF UNCERTAIN MEDICAL TERMS ===\")\n",
        "for example in uncertain_examples:\n",
        "    print(example)\n",
        "\n",
        "print(\"\\n=== EFFICIENCY ===\")\n",
        "print(f\"Total processing time: {processing_time:.2f} seconds\")\n",
        "\n",
        "print(\"\\n=== EVALUATION METRICS ===\")\n",
        "print(\"\\nNegation Metrics:\")\n",
        "print(f\"Precision: {prec_neg:.2f}, Recall: {rec_neg:.2f}, F1: {f1_neg:.2f}\")\n",
        "print(\"\\nUncertainty Metrics:\")\n",
        "print(f\"Precision: {prec_unc:.2f}, Recall: {rec_unc:.2f}, F1: {f1_unc:.2f}\")\n",
        "print(\"\\nCombined (Negation + Uncertainty) Metrics:\")\n",
        "print(f\"Precision: {prec_comb:.2f}, Recall: {rec_comb:.2f}, F1: {f1_comb:.2f}\")"
      ],
      "metadata": {
        "id": "oXK2hlkOg8ry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0323b659-3133-45dd-c005-e6af284c83bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SpaCy model...\n",
            "Please upload your JSON file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bb02fc08-8736-4425-a641-dddff3a2e252\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bb02fc08-8736-4425-a641-dddff3a2e252\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving negacio_train_v2024.json to negacio_train_v2024 (22).json\n",
            "Processing 254 records...\n",
            "Processing record 1/254...\n",
            "Processing record 11/254...\n",
            "Processing record 21/254...\n",
            "Processing record 31/254...\n",
            "Processing record 41/254...\n",
            "Processing record 51/254...\n",
            "Processing record 61/254...\n",
            "Processing record 71/254...\n",
            "Processing record 81/254...\n",
            "Processing record 91/254...\n",
            "Processing record 101/254...\n",
            "Processing record 111/254...\n",
            "Processing record 121/254...\n",
            "Processing record 131/254...\n",
            "Processing record 141/254...\n",
            "Processing record 151/254...\n",
            "Processing record 161/254...\n",
            "Processing record 171/254...\n",
            "Processing record 181/254...\n",
            "Processing record 191/254...\n",
            "Processing record 201/254...\n",
            "Processing record 211/254...\n",
            "Processing record 221/254...\n",
            "Processing record 231/254...\n",
            "Processing record 241/254...\n",
            "Processing record 251/254...\n",
            "\n",
            "=== STATISTICS ===\n",
            "\n",
            "Negation Cue Frequencies (affecting medical terms):\n",
            "no: 466\n",
            "ni: 448\n",
            "sin: 142\n",
            "sin signos de: 23\n",
            "sense: 11\n",
            "descarta: 8\n",
            "negativa: 4\n",
            "ausencia de: 4\n",
            "negativo: 4\n",
            "nada: 3\n",
            "sin evidencia de: 2\n",
            "no presenta: 2\n",
            "sin cambios: 1\n",
            "libre de: 1\n",
            "no se observa: 1\n",
            "no muestra: 1\n",
            "descartando: 1\n",
            "\n",
            "Uncertainty Cue Frequencies (affecting medical terms):\n",
            "probable: 24\n",
            "posible: 12\n",
            "aparentemente: 9\n",
            "compatible con: 3\n",
            "sospecha de: 3\n",
            "parece: 3\n",
            "probablemente: 2\n",
            "sugestivo de: 1\n",
            "possible: 1\n",
            "posiblemente: 1\n",
            "\n",
            "Medical Terms Frequencies:\n",
            "parto: 250\n",
            "renal: 198\n",
            "aguda: 154\n",
            "cronica: 97\n",
            "arterial: 85\n",
            "fractura: 84\n",
            "hipertension: 75\n",
            "insuficiencia: 65\n",
            "interna: 64\n",
            "parcial: 50\n",
            "ictus: 44\n",
            "estenosis: 43\n",
            "protesis: 41\n",
            "cerebral: 41\n",
            "eutocico: 40\n",
            "endovenosa: 36\n",
            "membranas: 33\n",
            "infarto: 33\n",
            "apendicectomia: 27\n",
            "nefrectomia: 27\n",
            "laparoscopica: 26\n",
            "cadera: 21\n",
            "episiotomia: 20\n",
            "rotura: 19\n",
            "renales: 18\n",
            "cistoscopia: 16\n",
            "isquemico: 16\n",
            "bloqueo: 14\n",
            "uretra: 14\n",
            "quistes: 12\n",
            "prematuro: 12\n",
            "herniorrafia: 11\n",
            "mandibular: 10\n",
            "sensitiva: 9\n",
            "diverticulosis: 8\n",
            "colangitis: 8\n",
            "neuropatia: 7\n",
            "axonal: 7\n",
            "trombectomia: 7\n",
            "colelitiasis: 6\n",
            "multifactorial: 6\n",
            "gastroenteritis: 5\n",
            "cordectomia: 4\n",
            "polineuropatia: 4\n",
            "uretrotomia: 3\n",
            "angiomiolipoma: 3\n",
            "microcirugia: 3\n",
            "endolaringea: 3\n",
            "fibrinolisis: 3\n",
            "mielopatia: 2\n",
            "auriculoventricular: 1\n",
            "\n",
            "Medical Suffix Terms Frequencies:\n",
            "estenosis: 43\n",
            "apendicectomia: 27\n",
            "nefrectomia: 27\n",
            "anatomia: 24\n",
            "cardiopatia: 20\n",
            "episiotomia: 20\n",
            "colecistectomia: 12\n",
            "faquectomia: 11\n",
            "mastectomia: 11\n",
            "adenopatia: 10\n",
            "amigdalectomia: 9\n",
            "polipectomia: 9\n",
            "diverticulosis: 8\n",
            "osteotomia: 8\n",
            "linfadenectomia: 8\n",
            "neuropatia: 7\n",
            "hepatopatia: 7\n",
            "trombectomia: 7\n",
            "histerectomia: 7\n",
            "miopatia: 7\n",
            "broncopatia: 7\n",
            "retinopatia: 6\n",
            "artropatia: 6\n",
            "esfinterotomia: 6\n",
            "nefrostomia: 6\n",
            "laparotomia: 6\n",
            "nefropatia: 6\n",
            "hemicolectomia: 5\n",
            "encefalopatia: 5\n",
            "traqueostomia: 5\n",
            "gammapatia: 5\n",
            "mastetctomia: 5\n",
            "cordectomia: 4\n",
            "salpinguectomia: 4\n",
            "anexectomia: 4\n",
            "polineuropatia: 4\n",
            "tumorectomia: 4\n",
            "cistectomia: 4\n",
            "espondilosis: 4\n",
            "melanosis: 4\n",
            "adenoidectomia: 4\n",
            "uretrotomia: 3\n",
            "arteriovenostomia: 3\n",
            "tuberculosis: 3\n",
            "laringectomia: 3\n",
            "meniscectomia: 3\n",
            "hepatectomia: 3\n",
            "segmentectomia: 3\n",
            "discopatia: 3\n",
            "gastrectomia: 3\n",
            "laminectomia: 3\n",
            "neumopatia: 3\n",
            "tiroidectomia: 3\n",
            "arteriopatia: 3\n",
            "leucoencefalopatia: 3\n",
            "ureterostomia: 3\n",
            "mielopatia: 2\n",
            "uropatia: 2\n",
            "miocardiopatia: 2\n",
            "glosectomia: 2\n",
            "traqueotomia: 2\n",
            "adrenalectomia: 2\n",
            "cianosis: 2\n",
            "angiopatia: 2\n",
            "radiculopatia: 2\n",
            "omentectomia: 2\n",
            "litotomia: 2\n",
            "uretrtomia: 1\n",
            "uretrostomia: 1\n",
            "amniotomia: 1\n",
            "neuronopatia: 1\n",
            "milopatia: 1\n",
            "poliradiculopatia: 1\n",
            "polirradiculopatia: 1\n",
            "gastropatia: 1\n",
            "papilotomia: 1\n",
            "traquesotomia: 1\n",
            "colectomia: 1\n",
            "colostomia: 1\n",
            "mas.anatomia: 1\n",
            "microangiopatia: 1\n",
            "conectivopatia: 1\n",
            "osteoartropatia: 1\n",
            "hidrocelectomia: 1\n",
            "fasciotomia: 1\n",
            "vasculopatia: 1\n",
            "paratiroidectomia: 1\n",
            "parotidectomia: 1\n",
            "hemorroidectomia: 1\n",
            "capsulotomia: 1\n",
            "subestenosis: 1\n",
            "tumoectomia: 1\n",
            "varicectomia: 1\n",
            "lobectomia: 1\n",
            "flavectomia: 1\n",
            "foraminotomia: 1\n",
            "pneumopatia: 1\n",
            "sigmoidectomia: 1\n",
            "miomectomia: 1\n",
            "prostatectomia: 1\n",
            "postamigdalectomia: 1\n",
            "gastrogastrostomia: 1\n",
            "laringuectomia: 1\n",
            "hemimaxilectomia: 1\n",
            "secuestrectomia: 1\n",
            "hipovitaminosis: 1\n",
            "coronoriopatia: 1\n",
            "colecistectectomia: 1\n",
            "coclecistectomia: 1\n",
            "histerctomia: 1\n",
            "valvulopatia: 1\n",
            "hemimaxielctomia: 1\n",
            "cortiquectomia: 1\n",
            "artrotomia: 1\n",
            "retrectomia: 1\n",
            "periostomia: 1\n",
            "apatia: 1\n",
            "esplenectomia: 1\n",
            "safenectomia: 1\n",
            "apendicetomia: 1\n",
            "\n",
            "=== EXTENDED UMLS TERMS ===\n",
            "adenoidectomia (total frequency: 4)\n",
            "adenopatia (total frequency: 10)\n",
            "adrenalectomia (total frequency: 2)\n",
            "aguda (total frequency: 154)\n",
            "amigdalectomia (total frequency: 9)\n",
            "amniotomia (total frequency: 1)\n",
            "anatomia (total frequency: 24)\n",
            "anexectomia (total frequency: 4)\n",
            "angiomiolipoma (total frequency: 3)\n",
            "angiopatia (total frequency: 2)\n",
            "apatia (total frequency: 1)\n",
            "apendicectomia (total frequency: 54)\n",
            "apendicetomia (total frequency: 1)\n",
            "arterial (total frequency: 85)\n",
            "arteriopatia (total frequency: 3)\n",
            "arteriovenostomia (total frequency: 3)\n",
            "artropatia (total frequency: 6)\n",
            "artrotomia (total frequency: 1)\n",
            "auriculoventricular (total frequency: 1)\n",
            "axonal (total frequency: 7)\n",
            "bloqueo (total frequency: 14)\n",
            "broncopatia (total frequency: 7)\n",
            "cadera (total frequency: 21)\n",
            "capsulotomia (total frequency: 1)\n",
            "cardiopatia (total frequency: 20)\n",
            "cerebral (total frequency: 41)\n",
            "cianosis (total frequency: 2)\n",
            "cistectomia (total frequency: 4)\n",
            "cistoscopia (total frequency: 16)\n",
            "coclecistectomia (total frequency: 1)\n",
            "colangitis (total frequency: 8)\n",
            "colecistectectomia (total frequency: 1)\n",
            "colecistectomia (total frequency: 12)\n",
            "colectomia (total frequency: 1)\n",
            "colelitiasis (total frequency: 6)\n",
            "colostomia (total frequency: 1)\n",
            "conectivopatia (total frequency: 1)\n",
            "cordectomia (total frequency: 8)\n",
            "coronoriopatia (total frequency: 1)\n",
            "cortiquectomia (total frequency: 1)\n",
            "cronica (total frequency: 97)\n",
            "discopatia (total frequency: 3)\n",
            "diverticulosis (total frequency: 16)\n",
            "déficit (total frequency: 0)\n",
            "encefalopatia (total frequency: 5)\n",
            "endolaringea (total frequency: 3)\n",
            "endovenosa (total frequency: 36)\n",
            "episiotomia (total frequency: 40)\n",
            "esfinterotomia (total frequency: 6)\n",
            "esplenectomia (total frequency: 1)\n",
            "espondilosis (total frequency: 4)\n",
            "estenosis (total frequency: 86)\n",
            "eutocico (total frequency: 40)\n",
            "faquectomia (total frequency: 11)\n",
            "fasciotomia (total frequency: 1)\n",
            "fibrinolisis (total frequency: 3)\n",
            "flavectomia (total frequency: 1)\n",
            "foraminotomia (total frequency: 1)\n",
            "fractura (total frequency: 84)\n",
            "gammapatia (total frequency: 5)\n",
            "gastrectomia (total frequency: 3)\n",
            "gastroenteritis (total frequency: 5)\n",
            "gastrogastrostomia (total frequency: 1)\n",
            "gastropatia (total frequency: 1)\n",
            "glosectomia (total frequency: 2)\n",
            "hemicolectomia (total frequency: 5)\n",
            "hemimaxielctomia (total frequency: 1)\n",
            "hemimaxilectomia (total frequency: 1)\n",
            "hemorroidectomia (total frequency: 1)\n",
            "hepatectomia (total frequency: 3)\n",
            "hepatopatia (total frequency: 7)\n",
            "herniorrafia (total frequency: 11)\n",
            "hidrocelectomia (total frequency: 1)\n",
            "hipertension (total frequency: 75)\n",
            "hipovitaminosis (total frequency: 1)\n",
            "histerctomia (total frequency: 1)\n",
            "histerectomia (total frequency: 7)\n",
            "ictus (total frequency: 44)\n",
            "infarto (total frequency: 33)\n",
            "insuficiencia (total frequency: 65)\n",
            "interna (total frequency: 64)\n",
            "isquemico (total frequency: 16)\n",
            "lactancia materna (total frequency: 0)\n",
            "laminectomia (total frequency: 3)\n",
            "laparoscopica (total frequency: 26)\n",
            "laparotomia (total frequency: 6)\n",
            "laringectomia (total frequency: 3)\n",
            "laringuectomia (total frequency: 1)\n",
            "leucoencefalopatia (total frequency: 3)\n",
            "linfadenectomia (total frequency: 8)\n",
            "litotomia (total frequency: 2)\n",
            "lobectomia (total frequency: 1)\n",
            "mandibular (total frequency: 10)\n",
            "mas.anatomia (total frequency: 1)\n",
            "mastectomia (total frequency: 11)\n",
            "mastetctomia (total frequency: 5)\n",
            "melanosis (total frequency: 4)\n",
            "membranas (total frequency: 33)\n",
            "meniscectomia (total frequency: 3)\n",
            "microangiopatia (total frequency: 1)\n",
            "microcirugia (total frequency: 3)\n",
            "mielopatia (total frequency: 4)\n",
            "milopatia (total frequency: 1)\n",
            "miocardiopatia (total frequency: 2)\n",
            "miomectomia (total frequency: 1)\n",
            "miopatia (total frequency: 7)\n",
            "multifactorial (total frequency: 6)\n",
            "nefrectomia (total frequency: 54)\n",
            "nefropatia (total frequency: 6)\n",
            "nefrostomia (total frequency: 6)\n",
            "neumopatia (total frequency: 3)\n",
            "neuronopatia (total frequency: 1)\n",
            "neuropatia (total frequency: 14)\n",
            "omentectomia (total frequency: 2)\n",
            "osteoartropatia (total frequency: 1)\n",
            "osteotomia (total frequency: 8)\n",
            "papilotomia (total frequency: 1)\n",
            "paratiroidectomia (total frequency: 1)\n",
            "parcial (total frequency: 50)\n",
            "parotidectomia (total frequency: 1)\n",
            "parto (total frequency: 250)\n",
            "periostomia (total frequency: 1)\n",
            "pneumopatia (total frequency: 1)\n",
            "polineuropatia (total frequency: 8)\n",
            "polipectomia (total frequency: 9)\n",
            "poliradiculopatia (total frequency: 1)\n",
            "polirradiculopatia (total frequency: 1)\n",
            "postamigdalectomia (total frequency: 1)\n",
            "prematuro (total frequency: 12)\n",
            "primer grado (total frequency: 0)\n",
            "prostatectomia (total frequency: 1)\n",
            "protesis (total frequency: 41)\n",
            "quistes (total frequency: 12)\n",
            "radiculopatia (total frequency: 2)\n",
            "renal (total frequency: 198)\n",
            "renales (total frequency: 18)\n",
            "retinopatia (total frequency: 6)\n",
            "retrectomia (total frequency: 1)\n",
            "rotura (total frequency: 19)\n",
            "safenectomia (total frequency: 1)\n",
            "salpinguectomia (total frequency: 4)\n",
            "secuestrectomia (total frequency: 1)\n",
            "segmentectomia (total frequency: 3)\n",
            "segundo grado (total frequency: 0)\n",
            "sensitiva (total frequency: 9)\n",
            "sigmoidectomia (total frequency: 1)\n",
            "subestenosis (total frequency: 1)\n",
            "tiroidectomia (total frequency: 3)\n",
            "traqueostomia (total frequency: 5)\n",
            "traqueotomia (total frequency: 2)\n",
            "traquesotomia (total frequency: 1)\n",
            "trombectomia (total frequency: 14)\n",
            "tuberculosis (total frequency: 3)\n",
            "tumoectomia (total frequency: 1)\n",
            "tumorectomia (total frequency: 4)\n",
            "ureterostomia (total frequency: 3)\n",
            "uretra (total frequency: 14)\n",
            "uretrostomia (total frequency: 1)\n",
            "uretrotomia (total frequency: 6)\n",
            "uretrtomia (total frequency: 1)\n",
            "uropatia (total frequency: 2)\n",
            "valvulopatia (total frequency: 1)\n",
            "varicectomia (total frequency: 1)\n",
            "vasculopatia (total frequency: 1)\n",
            "\n",
            "=== EXAMPLES OF NEGATED MEDICAL TERMS ===\n",
            "'diverticulosis extensa insuficiencia renal cronica colelitiasis' (term: diverticulosis)\n",
            "'diverticulosis extensa insuficiencia renal cronica colelitiasis antecedentes quirurgicos' (term: insuficiencia)\n",
            "'diverticulosis extensa insuficiencia renal cronica colelitiasis antecedentes quirurgicos :' (term: renal)\n",
            "'diverticulosis extensa insuficiencia renal cronica colelitiasis antecedentes quirurgicos : exeresis' (term: cronica)\n",
            "'diverticulosis extensa insuficiencia renal cronica colelitiasis antecedentes quirurgicos : exeresis de' (term: colelitiasis)\n",
            "'lesiones cutaneas con anestesia local protesis total de cadera cordectomia herniorrafia' (term: protesis)\n",
            "'anestesia local protesis total de cadera cordectomia herniorrafia inguinal proces actual' (term: cadera)\n",
            "'local protesis total de cadera cordectomia herniorrafia inguinal proces actual varon' (term: cordectomia)\n",
            "'protesis total de cadera cordectomia herniorrafia inguinal proces actual varon de' (term: herniorrafia)\n",
            "'de hematuria macroscopica se realiza cistoscopia que es negativa para lesiones' (term: cistoscopia)\n",
            "'lesiones malignas pero se objetiva estenosis de uretra .' (term: estenosis)\n",
            "'pero se objetiva estenosis de uretra .' (term: uretra)\n",
            "'que confirma la existencia de estenosis a nivel d uretra bulbar' (term: estenosis)\n",
            "'de estenosis a nivel d uretra bulbar por lo que se' (term: uretra)\n",
            "'la uretrografia retrograda muestra una uretra anterior con dos estenosis focales' (term: uretra)\n",
            "'una uretra anterior con dos estenosis focales a nivel de uretra' (term: estenosis)\n",
            "'estenosis focales a nivel de uretra peneana y bulbar , aunque' (term: uretra)\n",
            "'la uretrografia miccional muestra una uretra prostatica dilatada , sin claras' (term: uretra)\n",
            "'prostatica dilatada , sin claras estenosis focales confirmandose la existencia de' (term: estenosis)\n",
            "'la existencia de las dos estenosis de uretra anterior descritas previamente' (term: estenosis)\n",
            "\n",
            "=== EXAMPLES OF UNCERTAIN MEDICAL TERMS ===\n",
            "'elevacion de reactantes de fase aguda compatible con cuadro infeccioso gastrointestinal' (term: aguda)\n",
            "'sospecha de apendicitis aguda .' (term: aguda)\n",
            "'bajo la sospecha de posible polineuropatia multifactorial toxicometabolica y carencial yo' (term: polineuropatia)\n",
            "'la sospecha de posible polineuropatia multifactorial toxicometabolica y carencial yo mielopatia' (term: multifactorial)\n",
            "'placenta y membranas aparentemente integras .' (term: membranas)\n",
            "', como probables causas del ictus isquemico .' (term: ictus)\n",
            "'como probables causas del ictus isquemico .' (term: isquemico)\n",
            "'pared toracica anterior izq 11 insuficiencia renal que parece moderada proces' (term: insuficiencia)\n",
            "'toracica anterior izq 11 insuficiencia renal que parece moderada proces actual' (term: renal)\n",
            "'7.227.27 eb 0.92.9 placenta y membranas aparentemente integras .' (term: membranas)\n",
            "'proteinuria nefrotica y doble sistema renal izquierdo sugestivo de dudosa duplicacion' (term: renal)\n",
            "'antecedentes patologicos   hipertension arterial probablemente esencial en tratamiento' (term: hipertension)\n",
            "'antecedentes patologicos   hipertension arterial probablemente esencial en tratamiento con' (term: arterial)\n",
            "'derecha en probable relacion a infarto establecido en territorio de la' (term: infarto)\n",
            "'del contraste i.v en fase arterial y lavado , compatible con' (term: arterial)\n",
            "'complementarias rm cerebral informada con infarto agudo con posible patron ateroembolico' (term: infarto)\n",
            "'revision de placenta y membranas aparentemente integras .' (term: membranas)\n",
            "'logopedia , atribuït a possible ictus .' (term: ictus)\n",
            "'posible ictus isquemico transitorio en 2007 con' (term: ictus)\n",
            "'posible ictus isquemico transitorio en 2007 con afeccion' (term: isquemico)\n",
            "\n",
            "=== EFFICIENCY ===\n",
            "Total processing time: 34.21 seconds\n",
            "\n",
            "=== EVALUATION METRICS ===\n",
            "\n",
            "Negation Metrics:\n",
            "Precision: 0.00, Recall: 0.00, F1: 0.00\n",
            "\n",
            "Uncertainty Metrics:\n",
            "Precision: 0.00, Recall: 0.00, F1: 0.00\n",
            "\n",
            "Combined (Negation + Uncertainty) Metrics:\n",
            "Precision: 0.00, Recall: 0.00, F1: 0.00\n"
          ]
        }
      ]
    }
  ]
}